<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-09-18T23:17:21+02:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Chema Guerra</title><subtitle>This is a coding blog where I post about the various projects that I am currently working on, both at work, or as an independent researcher. Most content here revolves around computer graphics and physics simulation.</subtitle><author><name>Chema Guerra</name></author><entry><title type="html">Procedural Island Generation (VI)</title><link href="http://0.0.0.0:4000/2025/09/28/procedural-island-generation-vi.html" rel="alternate" type="text/html" title="Procedural Island Generation (VI)" /><published>2025-09-28T00:00:00+02:00</published><updated>2025-09-28T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/28/procedural-island-generation-vi</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/28/procedural-island-generation-vi.html"><![CDATA[<p><img src="/uploads/2025/procedural-island-generation-vi/chemaguerra-island-final-beauty.png" alt="Final rendered island with all systems" /></p>

<p>This is the final installment of our procedural island generation series. After building the mesh foundation (<a href="/2025/09/07/procedural-island-generation-i.html">Part I</a>), painting elevation hints (<a href="/2025/09/10/procedural-island-generation-ii.html">Part II</a>), adding mountain detail (<a href="/2025/09/13/procedural-island-generation-iii.html">Part III</a>), simulating hydrology (<a href="/2025/09/16/procedural-island-generation-iv.html">Part IV</a>), and colouring the terrain with our biome ramp (<a href="/2025/09/19/procedural-island-generation-v.html">Part V</a>), it is time to package the result. CartoKit finishes by baking the terrain into a compact mesh, visualising it through an egui viewer, and exporting artefacts for other tools.</p>

<p>The journey from mathematical representation to visual output ends with three components:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">Terrain::from_terrain</code></strong> – a baked mesh carrying elevation, moisture, biome, and river metadata.</li>
  <li><strong>The debug renderer &amp; viewer</strong> – CPU rasterisers that turn the data into diagnostic images.</li>
  <li><strong>Export helpers</strong> – GLB export, PNG captures, and GIF generation built on the same primitives.</li>
</ol>

<p>Let’s look at each piece.</p>

<h2 id="baked-terrain-output">Baked Terrain Output</h2>

<p><code class="language-plaintext highlighter-rouge">Terrain::from_terrain</code> (<code class="language-plaintext highlighter-rouge">src/terrain.rs:368</code>) distils the incremental <code class="language-plaintext highlighter-rouge">TerrainBuilder</code> state into a reusable asset. The bake step:</p>

<ul>
  <li>Keeps only faces whose centroids lie inside the unit square, trimming the guard ring used during generation.</li>
  <li>Copies vertex elevation and moisture into mesh attributes so downstream tools can query them directly.</li>
  <li>Tags every face with average elevation, a <code class="language-plaintext highlighter-rouge">TerrainType</code> (land vs. ocean), and the coarse <code class="language-plaintext highlighter-rouge">BiomeType</code> classification introduced in Part V.</li>
  <li>Marks edges as regular, coastline, or river and stores river-flow magnitudes when a <code class="language-plaintext highlighter-rouge">RiverSystem</code> is present.</li>
</ul>

<p>The baked mesh is still a <code class="language-plaintext highlighter-rouge">TopoMesh</code>, meaning we retain the halfedge connectivity that made the earlier stages convenient. When you call <code class="language-plaintext highlighter-rouge">Terrain::from_terrain(&amp;builder)</code>, you get a self-contained structure that is ready for export or further processing without touching the heavy generation code again.</p>

<h2 id="cpu-debug-renderer">CPU Debug Renderer</h2>

<p>All of CartoKit’s imagery is rendered on the CPU. The <code class="language-plaintext highlighter-rouge">cartokit::debug</code> module contains a suite of rasterisers—triangle fill, watertight line drawing, paint-map sampling, rainfall heatmaps—that output directly into <code class="language-plaintext highlighter-rouge">image::RgbaImage</code> buffers. The viewer’s <code class="language-plaintext highlighter-rouge">DisplayRenderer</code> (<code class="language-plaintext highlighter-rouge">examples/viewer/display_modes.rs</code>) wires those helpers together:</p>

<ul>
  <li>Mesh modes (<code class="language-plaintext highlighter-rouge">SeedPoints</code>, <code class="language-plaintext highlighter-rouge">Delaunay</code>, <code class="language-plaintext highlighter-rouge">Voronoi</code>, <code class="language-plaintext highlighter-rouge">Quads</code>, <code class="language-plaintext highlighter-rouge">FinalTriangulation</code>) call <code class="language-plaintext highlighter-rouge">draw_topokit_mesh</code> with optional vertex overlays.</li>
  <li>Scalar fields (<code class="language-plaintext highlighter-rouge">TriangleElevation</code>, <code class="language-plaintext highlighter-rouge">DistanceField</code>, <code class="language-plaintext highlighter-rouge">Rainfall</code>, <code class="language-plaintext highlighter-rouge">Humidity</code>, <code class="language-plaintext highlighter-rouge">RiverFlow</code>, <code class="language-plaintext highlighter-rouge">Biome</code>) delegate to <code class="language-plaintext highlighter-rouge">draw_triangles_opt</code>/<code class="language-plaintext highlighter-rouge">draw_regions_opt</code> with palette swaps.</li>
  <li>Noise visualisations reuse the same pipeline, just swapping in different <code class="language-plaintext highlighter-rouge">TriangleProperty</code> variants.</li>
</ul>

<p>Because everything renders to software images, the viewer behaves the same on every platform, and the exported screenshots and GIFs are bit-for-bit identical to what you see on screen.</p>

<h2 id="interactive-viewer">Interactive Viewer</h2>

<p>The <code class="language-plaintext highlighter-rouge">cartokit_viewer</code> example wraps those images in an egui/eframe interface:</p>

<ul>
  <li>The right-hand parameter panel exposes seeds, Bridson separation, rainfall, and river controls, regenerating the terrain whenever you tweak them.</li>
  <li>The “Paint Terrain” mode lets you brush elevation hints onto the 128×128 paint map; the next regeneration integrates those hints into the terrain.</li>
  <li>Display modes cover the full pipeline: seed classification, mesh structure, mountain distance fields, rainfall, humidity, river diagnostics, biome colours, and the final shaded map.</li>
  <li>Animation tools (<code class="language-plaintext highlighter-rouge">examples/viewer/animation.rs</code>) let you scrub parameters over time and queue frame exports.</li>
</ul>

<p>Each frame, the viewer renders the active mode into an <code class="language-plaintext highlighter-rouge">RgbaImage</code>, uploads it as an egui texture, and then recycles the same image for exports. There is no separate rendering path—what you export is exactly what you preview.</p>

<h2 id="export-helpers">Export Helpers</h2>

<p>Three helpers in <code class="language-plaintext highlighter-rouge">examples/viewer/export.rs</code> turn the baked data into files:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 1. Bake + GLB export via MeshKit</span>
<span class="nn">display</span><span class="p">::</span><span class="nf">export_mesh</span><span class="p">(</span><span class="o">&amp;</span><span class="n">terrain_builder</span><span class="p">,</span> <span class="n">seed</span><span class="p">);</span>

<span class="c1">// 2. One PNG per display mode</span>
<span class="nn">display</span><span class="p">::</span><span class="nf">export_all_images</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">modes</span><span class="p">,</span> <span class="p">|</span><span class="n">mode</span><span class="p">|</span> <span class="n">renderer</span><span class="nf">.render</span><span class="p">(</span><span class="n">mode</span><span class="p">));</span>

<span class="c1">// 3. Thumbnail tiles for quick comparisons</span>
<span class="nn">display</span><span class="p">::</span><span class="nf">export_all_images_tile</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">modes</span><span class="p">,</span> <span class="p">|</span><span class="n">mode</span><span class="p">|</span> <span class="n">renderer</span><span class="nf">.render</span><span class="p">(</span><span class="n">mode</span><span class="p">));</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">export_mesh</code> clones the baked <code class="language-plaintext highlighter-rouge">Terrain</code>, rescales coordinates for GLTF’s Y‑up convention, and calls <code class="language-plaintext highlighter-rouge">meshkit::io::save_mesh</code> to produce a <code class="language-plaintext highlighter-rouge">.glb</code> file that loads cleanly in Blender or other viewers.</li>
  <li><code class="language-plaintext highlighter-rouge">export_all_images</code> walks every display mode and drops the rendered PNGs into <code class="language-plaintext highlighter-rouge">exports/images_seed_&lt;seed&gt;/</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">export_all_images_tile</code> cuts out the top-left 1/8×1/8 tile from each image—handy for diffing or documentation thumbnails.</li>
</ul>

<p>For animated parameter studies, <code class="language-plaintext highlighter-rouge">examples/viewer/gif_export.rs</code> converts a list of pre-rendered frames into a looping GIF, with options for downsampling, FPS, and output directory naming.</p>

<h2 id="performance-snapshot">Performance Snapshot</h2>

<p>Generation times were covered in the earlier posts (≈80 ms for the default 27 K-site map on a modern desktop). The finishing steps add little overhead:</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Time (2048×2048 render)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Terrain::from_terrain</code> bake</td>
      <td>~6 ms</td>
    </tr>
    <tr>
      <td>CPU render – biome view</td>
      <td>~8 ms</td>
    </tr>
    <tr>
      <td>GLB export (<code class="language-plaintext highlighter-rouge">meshkit::io::save_mesh</code>)</td>
      <td>~15 ms</td>
    </tr>
    <tr>
      <td>PNG capture per display mode</td>
      <td>5–10 ms</td>
    </tr>
    <tr>
      <td>GIF encoding (20 frames @ 1024²)</td>
      <td>~9 ms</td>
    </tr>
  </tbody>
</table>

<p>Numbers vary with map resolution and active mode (mesh overlays with AA lines take longer than simple heatmaps), but everything remains comfortably interactive.</p>

<h2 id="future-directions">Future Directions</h2>

<p>With a solid foundation in place, the obvious extensions are clear:</p>

<ul>
  <li><strong>GPU shading</strong> – real-time lighting, water reflections, and atmospheric effects on top of the baked mesh.</li>
  <li><strong>Mesh decimation</strong> – level-of-detail generation or streaming tiles for massive worlds.</li>
  <li><strong>Additional exporters</strong> – heightmaps, splatmaps, or direct integrations for Unity/Unreal/Godot.</li>
  <li><strong>Dynamic overlays</strong> – weather, vegetation instancing, or settlement placement driven by the existing attributes.</li>
</ul>

<p>These items all build on the baked <code class="language-plaintext highlighter-rouge">Terrain</code> structure and export scaffolding we now have.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Through six posts we moved from random seeds to a fully packaged island:</p>

<ol>
  <li>Poisson disk sampling and dual meshes establish the geometric scaffold.</li>
  <li>A paint map and layered noise sculpt elevation.</li>
  <li>Hydrology adds rivers and erosion cues.</li>
  <li>A simple elevation/moisture colormap paints believable biomes.</li>
  <li>The baked <code class="language-plaintext highlighter-rouge">Terrain</code>, debug renderer, and export helpers ship the result.</li>
</ol>

<p>The complete system generates a richly annotated island in under a tenth of a second and provides everything you need to inspect, tweak, and export it. The modular architecture welcomes experimentation—swap out any component and the rest of the pipeline keeps working.</p>

<p>Thanks for following along. I hope CartoKit inspires your own explorations into procedural worlds.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://github.com/brash-and-plucky/meshkit">MeshKit</a> – Halfedge library used throughout the project</li>
  <li><a href="https://www.egui.rs/">EGUI</a> – Immediate-mode GUI powering the viewer</li>
  <li><a href="https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.pdf">glTF 2.0</a> – Asset format we export to</li>
  <li><a href="https://www.redblobgames.com/maps/mapgen2/">Red Blob Games: Polygonal Map Generation</a> – Foundational reading for the dual-mesh approach</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="rendering" /><category term="gltf" /><category term="mesh" /><category term="export" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-v.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-v.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Procedural Island Generation (V)</title><link href="http://0.0.0.0:4000/2025/09/25/procedural-island-generation-v.html" rel="alternate" type="text/html" title="Procedural Island Generation (V)" /><published>2025-09-25T00:00:00+02:00</published><updated>2025-09-25T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/25/procedural-island-generation-v</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/25/procedural-island-generation-v.html"><![CDATA[<figure>
  <img src="/uploads/2025/procedural-island-generation-v/chemaguerra-biome-distribution-full.png" alt="Biome distribution visualization" />
  <figcaption>Biome distribution visualization</figcaption>
</figure>

<p>This post is a direct continuation to <a href="/2025/09/16/procedural-island-generation-iv.html">Part IV</a>, where we simulated the hydrological cycle. Now we’ll paint our island with life, using elevation and moisture to generate realistic biome distributions.</p>

<p>Biomes emerge from the interplay of climate factors. Temperature drops with altitude, moisture varies with rainfall and proximity to water, and these gradients create distinct ecological zones. CartoKit distils that idea into two ingredients—elevation and moisture—and uses them to drive both a continuous colour ramp and a handful of coarse biome tags.</p>

<h2 id="biome-buckets">Biome Buckets</h2>

<p>CartoKit takes a deliberately small-step approach to biome classification. The exporter stores a coarse <code class="language-plaintext highlighter-rouge">BiomeType</code> per triangle so downstream tools can switch materials or spawn vegetation without re-running the full generator. The thresholds live inside <code class="language-plaintext highlighter-rouge">Terrain::classify_biome</code> (<code class="language-plaintext highlighter-rouge">src/terrain.rs:683</code>):</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">classify_biome</span><span class="p">(</span><span class="n">elevation</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span> <span class="n">moisture</span><span class="p">:</span> <span class="nb">f32</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">BiomeType</span> <span class="p">{</span>
    <span class="k">if</span> <span class="n">elevation</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">0.5</span> <span class="p">{</span>
        <span class="nn">BiomeType</span><span class="p">::</span><span class="n">DeepOcean</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">elevation</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="p">{</span>
        <span class="nn">BiomeType</span><span class="p">::</span><span class="n">ShallowOcean</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">elevation</span> <span class="o">&lt;</span> <span class="mf">0.01</span> <span class="p">{</span>
        <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Beach</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">elevation</span> <span class="o">&lt;</span> <span class="mf">0.3</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">moisture</span> <span class="o">&lt;</span> <span class="mf">0.3</span> <span class="p">{</span>
            <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Desert</span>
        <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">moisture</span> <span class="o">&lt;</span> <span class="mf">0.6</span> <span class="p">{</span>
            <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Grassland</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Forest</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">elevation</span> <span class="o">&lt;</span> <span class="mf">0.6</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">moisture</span> <span class="o">&lt;</span> <span class="mf">0.33</span> <span class="p">{</span>
            <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Tundra</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Taiga</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nn">BiomeType</span><span class="p">::</span><span class="n">Snow</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That yields the following buckets:</p>

<table>
  <thead>
    <tr>
      <th>Elevation band</th>
      <th>Moisture split</th>
      <th>BiomeType</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt; -0.5</code></td>
      <td>—</td>
      <td>Deep Ocean</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">[-0.5, 0)</code></td>
      <td>—</td>
      <td>Shallow Ocean</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">[0, 0.01)</code></td>
      <td>—</td>
      <td>Beach</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">[0.01, 0.3)</code></td>
      <td><code class="language-plaintext highlighter-rouge">&lt;0.3</code> Desert / <code class="language-plaintext highlighter-rouge">[0.3,0.6)</code> Grassland / <code class="language-plaintext highlighter-rouge">≥0.6</code> Forest</td>
      <td> </td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">[0.3, 0.6)</code></td>
      <td><code class="language-plaintext highlighter-rouge">&lt;0.33</code> Tundra / <code class="language-plaintext highlighter-rouge">≥0.33</code> Taiga</td>
      <td> </td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">≥0.6</code></td>
      <td>—</td>
      <td>Snow</td>
    </tr>
  </tbody>
</table>

<p>It is intentionally conservative: we only need enough variety to drive material swaps and the color ramp; anything more granular is left to future work.</p>

<h2 id="continuous-color-mapping">Continuous Color Mapping</h2>

<p>Visuals are driven by a continuous color function instead of discrete textures. The public API exposes <code class="language-plaintext highlighter-rouge">calculate_biome_color</code> (<code class="language-plaintext highlighter-rouge">src/biome.rs:15</code>), which maps elevation ∈ [-1,1] and moisture ∈ [0,1] to an RGBA value:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">fn</span> <span class="nf">calculate_biome_color</span><span class="p">(</span><span class="n">elevation</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span> <span class="n">moisture</span><span class="p">:</span> <span class="nb">f32</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="p">[</span><span class="nb">u8</span><span class="p">;</span> <span class="mi">4</span><span class="p">]</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">m</span> <span class="o">=</span> <span class="n">moisture</span><span class="nf">.clamp</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">);</span>
    <span class="k">let</span> <span class="n">e</span> <span class="o">=</span> <span class="n">elevation</span><span class="nf">.clamp</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">);</span>

    <span class="k">if</span> <span class="n">e</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">r</span> <span class="o">=</span> <span class="p">(</span><span class="mf">48.0</span> <span class="o">+</span> <span class="mf">48.0</span> <span class="o">*</span> <span class="n">e</span><span class="p">)</span><span class="nf">.max</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="mf">64.0</span> <span class="o">+</span> <span class="mf">64.0</span> <span class="o">*</span> <span class="n">e</span><span class="p">)</span><span class="nf">.max</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="mf">127.0</span> <span class="o">+</span> <span class="mf">127.0</span> <span class="o">*</span> <span class="n">e</span><span class="p">)</span><span class="nf">.max</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">255</span><span class="p">]</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">m</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">e</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">base_r</span> <span class="o">=</span> <span class="mf">210.0</span> <span class="o">-</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">m</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">base_g</span> <span class="o">=</span> <span class="mf">185.0</span> <span class="o">-</span> <span class="mf">45.0</span> <span class="o">*</span> <span class="n">m</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">base_b</span> <span class="o">=</span> <span class="mf">139.0</span> <span class="o">-</span> <span class="mf">45.0</span> <span class="o">*</span> <span class="n">m</span><span class="p">;</span>

        <span class="k">let</span> <span class="n">r</span> <span class="o">=</span> <span class="p">(</span><span class="mf">255.0</span> <span class="o">*</span> <span class="n">e</span> <span class="o">+</span> <span class="n">base_r</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">e</span><span class="p">))</span><span class="nf">.min</span><span class="p">(</span><span class="mf">255.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="mf">255.0</span> <span class="o">*</span> <span class="n">e</span> <span class="o">+</span> <span class="n">base_g</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">e</span><span class="p">))</span><span class="nf">.min</span><span class="p">(</span><span class="mf">255.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="mf">255.0</span> <span class="o">*</span> <span class="n">e</span> <span class="o">+</span> <span class="n">base_b</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">e</span><span class="p">))</span><span class="nf">.min</span><span class="p">(</span><span class="mf">255.0</span><span class="p">)</span> <span class="k">as</span> <span class="nb">u8</span><span class="p">;</span>
        <span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">255</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>Oceans fade from deep blue to turquoise as depth decreases.</li>
  <li>Land blends tan through green based on moisture, then ramps toward white as elevation climbs.</li>
</ul>

<p>Because the function is continuous, neighbouring triangles share smooth colour transitions without an explicit blending pass.</p>

<h2 id="2d-colormap-visualisation">2D Colormap Visualisation</h2>

<p>The viewer’s “Biome Colormap” debug mode is powered by <code class="language-plaintext highlighter-rouge">debug::biome_colormap::generate_colormap</code> (<code class="language-plaintext highlighter-rouge">src/debug/biome_colormap.rs:22</code>). It bakes the function above into a 2D texture where the X-axis is elevation and the Y-axis is moisture, letting us inspect the palette or export it for documentation.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="n">params</span> <span class="o">=</span> <span class="nn">ColormapParams</span><span class="p">::</span><span class="nf">default</span><span class="p">();</span>
<span class="k">let</span> <span class="n">image</span> <span class="o">=</span> <span class="nf">generate_colormap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">params</span><span class="p">);</span>
</code></pre></div></div>

<figure>
  <img src="/uploads/2025/procedural-island-generation-v/chemaguerra-biome-colormap-texture.png" alt="2D biome colormap texture" />
  <figcaption>2D biome colormap texture</figcaption>
</figure>

<h2 id="viewer-rendering">Viewer Rendering</h2>

<p>In the interactive viewer, the <code class="language-plaintext highlighter-rouge">Biome</code> and <code class="language-plaintext highlighter-rouge">BiomeWithRivers</code> display modes simply evaluate <code class="language-plaintext highlighter-rouge">calculate_biome_color</code> per triangle. The river overlay multiplies river flow onto the colour buffer so wet channels read clearly against the base biome ramp. There is no seasonal tinting, riparian override, or microclimate logic yet—those panels in the UI are placeholders for future experiments.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-v/chemaguerra-biome-transitions.png" alt="Biome display in the viewer" />
  <figcaption>Biome display in the viewer</figcaption>
</figure>

<h2 id="exported-attributes">Exported Attributes</h2>

<p>When we bake a <code class="language-plaintext highlighter-rouge">Terrain</code>, every face stores:</p>

<ul>
  <li>Average elevation and moisture (from vertex attributes).</li>
  <li><code class="language-plaintext highlighter-rouge">TerrainType</code> (land vs water) and the coarse <code class="language-plaintext highlighter-rouge">BiomeType</code> bucket from <code class="language-plaintext highlighter-rouge">classify_biome</code>.</li>
</ul>

<p>This information is embedded alongside the final triangulation mesh so external tools can colour-code or spawn assets without re-running CartoKit.</p>

<h2 id="performance-notes">Performance Notes</h2>

<p>Biome work piggybacks on data we already track:</p>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Time (27K triangles)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">calculate_biome_color</code> sampling in viewer</td>
      <td>~1 ms</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">classify_biome</code> during export</td>
      <td>~1 ms</td>
    </tr>
    <tr>
      <td>Colormap bake (64×64)</td>
      <td>~1 ms</td>
    </tr>
  </tbody>
</table>

<p>The runtime cost is negligible compared to hydrology—the colour ramp is just a handful of arithmetic ops per triangle.</p>

<h2 id="future-improvements">Future Improvements</h2>

<p>Several ideas from early design notes (temperature lapse rates, seasonal palettes, riparian upgrades, succession) have not been implemented yet. If we revisit biomes in the future, that backlog will drive the next iteration. For now the focus stays on delivering a stable colour ramp and a compact set of biome tags that downstream tools can consume reliably.</p>

<h2 id="next-steps">Next Steps</h2>

<p>Our island now pulses with life, painted in the colors of its ecosystems. Part VI, the final installment, dives into the baked <code class="language-plaintext highlighter-rouge">Terrain</code> output, the egui viewer, and the GLB/PNG/GIF export helpers that ship the result.</p>

<p>From the mathematical foundations to the living landscape, we’ve built a complete procedural island generation system. The conclusion awaits.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Biome#Whittaker_(1975)_biome-types">Whittaker’s Biome Classification</a> - Original biome model</li>
  <li><a href="https://www.redblobgames.com/maps/terrain-from-noise/#biomes">Vegetation Procedural Generation</a> - Red Blob Games biome techniques</li>
  <li><a href="https://en.wikipedia.org/wiki/Ecotone">Ecotones and Edge Effects</a> - Transition zone ecology</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="biomes" /><category term="ecosystems" /><category term="vegetation" /><category term="whittaker" /><summary type="html"><![CDATA[Biome distribution visualization]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iv.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iv.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Procedural Island Generation (IV)</title><link href="http://0.0.0.0:4000/2025/09/22/procedural-island-generation-iv.html" rel="alternate" type="text/html" title="Procedural Island Generation (IV)" /><published>2025-09-22T00:00:00+02:00</published><updated>2025-09-22T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/22/procedural-island-generation-iv</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/22/procedural-island-generation-iv.html"><![CDATA[<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-river-system-complete.png" alt="River system visualization" />
  <figcaption>River system visualization</figcaption>
</figure>

<p>This post is a direct continuation to <a href="/2025/09/13/procedural-island-generation-iii.html">Part III</a>, where we shaped our terrain with detailed elevation. Now we’ll simulate the complete hydrological cycle: rainfall patterns, river formation, and valley carving through erosion.</p>

<p>Water is the sculptor of landscapes. It carves valleys, deposits sediment, and fundamentally shapes terrain over geological time. While we can’t simulate millions of years of erosion, we can approximate the key processes to create believable drainage patterns.</p>

<h2 id="hydrological-color-gradients">Hydrological Color Gradients</h2>

<p>Before diving into the water simulation, let’s establish the color gradients used to visualize hydrological properties. Each gradient is carefully designed to represent its physical property intuitively:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-gradient-rainfall.png" alt="Rainfall gradient" />
  <figcaption>Rainfall: From arid brown through temperate green to wet blue</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-gradient-humidity.png" alt="Humidity gradient" />
  <figcaption>Humidity: Warm colors (dry) transitioning to cool colors (humid)</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-gradient-moisture.png" alt="Moisture gradient" />
  <figcaption>Moisture: Brown plateau for very dry, then yellow to green to blue</figcaption>
</figure>

<p>Key characteristics of each gradient:</p>
<ul>
  <li><strong>Rainfall</strong> uses earth tones transitioning to water colors, matching our intuition about dry vs wet climates</li>
  <li><strong>Humidity</strong> employs a temperature-based color scheme, with warm reds for dry air and cool purples for humid conditions</li>
  <li><strong>Moisture</strong> features a flat brown segment for arid regions before transitioning through yellow (semi-arid) to green (temperate) to blue (wet)</li>
</ul>

<h2 id="slope-assignment">Slope Assignment</h2>

<p>Before water can flow, we need to know which direction is downhill. Our slope assignment seeds a priority queue with ocean triangles (elevation &lt; -0.1) and grows inland. Whenever we pop a triangle, we force every unprocessed neighbour to drain back toward it by storing the twin halfedge as its downslope. The result is a directed acyclic graph that ultimately funnels all triangles to the coast—interior sinks are flattened rather than left as <code class="language-plaintext highlighter-rouge">None</code>.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/heightmap-downslope-composite.png" alt="Downslope connectivity between triangles" />
  <figcaption>Triangle mesh showing downslope connections from each triangle to its lowest neighbor (overlaid on terrain heightmap)</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-drainage-triangles.png" alt="Drainage flow between slope neighbors" />
  <figcaption>Drainage patterns emerging from slope neighbor relationships</figcaption>
</figure>

<h2 id="rainfall-distribution">Rainfall Distribution</h2>

<p>Rainfall drives the entire water cycle. Rather than uniform precipitation, we want realistic patterns influenced by terrain features. The key insight: mountains capture moisture from prevailing winds.</p>

<h3 id="orographic-rainfall">Orographic Rainfall</h3>

<p>When humid air masses encounter mountains, they’re forced upward where cooler temperatures cause condensation. This orographic lift creates heavy rainfall on windward slopes while leaving leeward sides dry—the famous “rain shadow” effect seen in mountain ranges worldwide.</p>

<p>Our implementation simulates this through a wind-ordered traversal of regions:</p>

<h4 id="wind-ordering">Wind Ordering</h4>

<p>First, we project each region’s position onto the wind direction vector:</p>

<p><strong>π</strong>ᵢ = <strong>p</strong>ᵢ · <strong>w</strong></p>

<p>where <strong>w</strong> = (cos θ, sin θ) for wind angle θ</p>

<p>Regions are then sorted by their projection value, ensuring upwind regions are processed before downwind ones. This ordering is crucial—it allows humidity to propagate naturally from region to region following the prevailing wind.</p>

<h4 id="humidity-propagation">Humidity Propagation</h4>

<p>For each region in wind-sorted order:</p>

<ol>
  <li><strong>Inherit humidity</strong> from upwind neighbors (average of already-processed neighbors)</li>
  <li><strong>Boundary regions</strong> (mesh edges) act as infinite moisture sources: h = 1.0</li>
  <li><strong>Evaporation</strong> over water adds moisture: h += ε × depth</li>
  <li><strong>Orographic precipitation</strong> occurs when humidity exceeds the lifting condensation level</li>
</ol>

<p>The key physics happens in step 4. When humid air encounters elevated terrain:</p>

<ul>
  <li>If humidity &gt; (1 - elevation), the excess condenses</li>
  <li>Rainfall = ρ × σ × excess (where ρ is raininess, σ is rain shadow strength)</li>
  <li>Remaining humidity = humidity - σ × excess</li>
</ul>

<p>This creates the rain shadow: as air masses cross mountains, they progressively lose moisture, leaving downstream regions increasingly dry.</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/rainfall-wind-angle.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Adjusting wind direction to control rain shadow patterns</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/rain-shadow.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Rain shadow effect parameter controlling moisture depletion</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-rainfall-distribution.png" alt="Rainfall distribution map" />
  <figcaption>Rainfall distribution map</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/evaporation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Evaporation rate affecting moisture availability</figcaption>
</figure>

<h3 id="humidity-propagation-1">Humidity Propagation</h3>

<p>There is no separate BFS pass—humidity and rainfall are produced together by <code class="language-plaintext highlighter-rouge">calculate_rainfall</code>. Boundary regions, evaporation over water, and orographic depletion are the only sources and sinks. Inland humidity therefore emerges from repeated inheritance in wind order, gradually decaying as rain shadows remove moisture.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-humidity-gradient.png" alt="Humidity gradient from coast" />
  <figcaption>Humidity gradient from coast</figcaption>
</figure>

<h2 id="river-flow-accumulation">River Flow Accumulation</h2>

<p>With slopes assigned, we simulate water flow by accumulating rainfall as it flows downhill:</p>

<h3 id="the-flow-algorithm">The Flow Algorithm</h3>

<p><code class="language-plaintext highlighter-rouge">assign_flow</code> works on top of the downslope graph produced earlier. We initialise every land triangle’s runoff from its local moisture (<code class="language-plaintext highlighter-rouge">flow * moisture²</code>), then walk triangles from the peaks down to the coast using the previously recorded visit order.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="o">&amp;</span><span class="n">t_tributary</span> <span class="k">in</span> <span class="n">triangle_order</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.rev</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t_trunk</span><span class="p">)</span> <span class="o">=</span> <span class="nf">downstream_triangle</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">t_tributary</span><span class="p">,</span> <span class="n">downslope_s</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">flow</span> <span class="o">=</span> <span class="n">triangle_flow</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">];</span>
        <span class="n">triangle_flow</span><span class="p">[</span><span class="n">t_trunk</span><span class="p">]</span> <span class="o">+=</span> <span class="n">flow</span><span class="p">;</span>      <span class="c1">// accumulate volume</span>
        <span class="n">halfedge_flow</span><span class="p">[</span><span class="n">s_flow</span><span class="p">]</span>      <span class="o">+=</span> <span class="n">flow</span><span class="p">;</span>  <span class="c1">// store river segment flow</span>

        <span class="k">if</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_trunk</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">]</span>
            <span class="o">&amp;&amp;</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>
        <span class="p">{</span>
            <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_trunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">];</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Because the traversal order already guarantees we see every tributary before its trunk, no priority queue or HashMap is required during accumulation. The companion array <code class="language-plaintext highlighter-rouge">halfedge_flow</code> gives us the per-edge flow rate that the viewer visualises.</p>

<p>(<code class="language-plaintext highlighter-rouge">downstream_triangle</code> in the snippet is just shorthand for the halfedge/twin lookup present in the code.)</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/flow-accumulation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Flow accumulation patterns as water drains downhill</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-flow-accumulation.png" alt="Flow accumulation visualization" />
  <figcaption>Flow accumulation visualization</figcaption>
</figure>

<h3 id="river-rendering">River Rendering</h3>

<p>Rivers appear where flow exceeds a threshold. We render them as blue lines with width proportional to flow:</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/rainfall-intensity.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Rainfall intensity directly affecting river flow volumes</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/min-flow-threshold.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: Minimum flow threshold for river visibility</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iv/river-width.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Interactive: River width scaling based on flow volume</figcaption>
</figure>

<p>The viewer’s debug renderer reads the accumulated <code class="language-plaintext highlighter-rouge">halfedge_flow</code> and draws anti-aliased line segments between triangle centroids. Colour intensity scales with flow volume, and for larger rivers we add a pair of offset strokes to fake extra width. No logarithmic scaling is involved yet—the visual thickness is driven by the raw flow magnitude and capped for stability.</p>

<h2 id="valley-carving">Valley Carving</h2>

<p>Rivers don’t just flow over terrain; they carve it. During flow accumulation we opportunistically enforce downhill consistency: whenever a tributary sits above sea level and drains into a taller neighbour, we simply flatten that neighbour to the tributary’s elevation.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_trunk</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">]</span>
    <span class="o">&amp;&amp;</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>
<span class="p">{</span>
    <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_trunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">modified_elevation</span><span class="p">[</span><span class="n">t_tributary</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>It’s a deliberately conservative carve—there’s no logarithmic depth, ridge-distance falloff, or post-processing smoothing yet—but it guarantees the final triangulation respects downhill flow without erasing coastlines.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-valley-carving.png" alt="Valley carving effect on terrain" />
  <figcaption>Valley carving effect on terrain</figcaption>
</figure>

<h2 id="moisture-calculation">Moisture Calculation</h2>

<p>Triangle moisture is simply the average of the rainfall at its three corners. This keeps moisture tightly coupled to the wind-and-orographic model and avoids extra passes:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="n">triangle_moisture</span> <span class="o">=</span> <span class="nf">assign_moisture</span><span class="p">(</span><span class="o">&amp;</span><span class="n">delaunay_mesh</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">region_rainfall</span><span class="p">);</span>
</code></pre></div></div>

<p>We do not yet boost moisture near rivers, so lush riparian corridors are driven indirectly by higher rainfall wherever drainage concentrates.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-moisture-distribution.png" alt="Moisture distribution map" />
  <figcaption>Moisture distribution map</figcaption>
</figure>

<h2 id="drainage-basins--lakes">Drainage Basins &amp; Lakes</h2>

<p>Watershed labelling and depression filling are still on the roadmap. Because every triangle currently has a downslope path to the ocean, interior lakes only appear where the base terrain already created them. Future work will revisit the downslope assignment to allow persistent sinks and to flood depressions up to their spill points.</p>

<h2 id="visual-results">Visual Results</h2>

<p>The complete hydrological system creates compelling, realistic terrain:</p>

<h3 id="river-networks">River Networks</h3>
<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-river-network-full.png" alt="Complete river network" />
  <figcaption>Complete river network</figcaption>
</figure>

<p>Natural branching patterns emerge from the flow accumulation algorithm.</p>

<h3 id="valley-systems">Valley Systems</h3>
<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-valley-detail.png" alt="Carved valley detail" />
  <figcaption>Carved valley detail</figcaption>
</figure>

<p>Rivers carve distinctive valleys while preserving ridge lines.</p>

<h3 id="moisture-patterns">Moisture Patterns</h3>
<figure>
  <img src="/uploads/2025/procedural-island-generation-iv/chemaguerra-moisture-vegetation.png" alt="Moisture creating vegetation zones" />
  <figcaption>Moisture creating vegetation zones</figcaption>
</figure>

<p>Moisture gradients create natural transitions between biomes.</p>

<h2 id="mathematical-validation">Mathematical Validation</h2>

<p>Two invariants keep the system stable:</p>

<ol>
  <li><strong>Directed flow graph</strong> – every triangle inherits a downslope edge that eventually reaches the ocean, so accumulated flow is monotonic and non-negative. There are no dangling sources once the ocean seed step completes.</li>
  <li><strong>Non-invasive carving</strong> – valley carving only ever lowers a downstream triangle to match a higher tributary that is still above sea level. Coastlines and underwater regions therefore remain untouched, preventing the algorithm from flooding the island accidentally.</li>
</ol>

<p>While the current runoff model uses heuristic scaling (<code class="language-plaintext highlighter-rouge">flow * moisture²</code>) rather than strict water balance, these constraints keep drainage visually coherent and numerically well-behaved.</p>

<h2 id="next-steps">Next Steps</h2>

<p>With water shaping our landscape, we’re ready to paint it with life. Part V will explore biome generation: how elevation, moisture, and temperature combine to create forests, deserts, tundra, and everything in between. We’ll see how the hydrological system we’ve built drives vegetation patterns and ecosystem boundaries.</p>

<p>The rivers we’ve carved don’t just shape terrain; they define where forests thrive, where deserts form, and where civilizations might emerge.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://ranmantaru.com/blog/2011/10/08/water-erosion-on-heightmap-terrain/">Hydraulic Erosion</a> - Implementation details</li>
  <li><a href="https://en.wikipedia.org/wiki/Stream_power">Stream Power Erosion</a> - Geological background</li>
  <li><a href="https://www.redblobgames.com/x/1743-planet-generation/">Drainage Basin Analysis</a> - Amit Patel’s watershed algorithms</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="hydrology" /><category term="rivers" /><category term="rainfall" /><category term="moisture" /><category term="erosion" /><summary type="html"><![CDATA[River system visualization]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iii.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iii.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Procedural Island Generation (III)</title><link href="http://0.0.0.0:4000/2025/09/17/procedural-island-generation-iii.html" rel="alternate" type="text/html" title="Procedural Island Generation (III)" /><published>2025-09-17T00:00:00+02:00</published><updated>2025-09-17T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/17/procedural-island-generation-iii</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/17/procedural-island-generation-iii.html"><![CDATA[<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-terrain-elevation-detailed.png" alt="Terrain elevation with noise layers" />
  <figcaption>Resulting terrain elevation with multi-scale noise layers and mountain peaks</figcaption>
</figure>

<p>This post continues from <a href="/2025/09/10/procedural-island-generation-ii.html">Part II</a>, where we established the paint map foundation and mountain ridge system. Now we’ll add detailed noise layers, distance-based mountain peaks, and do blending to create the final terrain elevation.</p>

<h2 id="paint-map-recap">Paint Map (recap)</h2>

<p>Before applying noise layers, we start with the foundation established in Part I - the paint map that defines our base land/water distribution:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-part-i.png" alt="Paint map from Part I" />
  <figcaption>The paint map from Part I - our starting elevation values before noise enhancement</figcaption>
</figure>

<p>For visualization throughout this post, we’ll be using the magma palette from matplotlib, which I patched to artificially darken the ocean areas to highlight the coastline:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-triangulated.png" alt="Paint map sampled at triangle centroids" />
  <figcaption>Paint map values sampled at the centroids of Delaunay triangles</figcaption>
</figure>

<p>Note that we’ll be sampling the paint map <em>per Delaunay triangle</em> (at each triangle’s centroid):</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-paint-map-triangulated-center.png" alt="Central portion of triangulated paint map" />
  <figcaption>Central portion of the above image showing the per-triangle sampling more clearly</figcaption>
</figure>

<p>Remember that the paint map provides the broad strokes: positive values for land, negative for ocean, with smooth transitions between them. Now we’ll enhance it with noise layers to create realistic terrain detail.</p>

<h2 id="multi-scale-noise-layers">Multi-Scale Noise Layers</h2>

<p>We will layer multiple octaves of Simplex noise at different frequencies over the broad strokes provided by the paint map. Each will contribute different detail scales to the final terrain.</p>

<p><a href="https://github.com/redblobgames/mapgen4">mapgen4</a> by <a href="https://x.com/redblobgames">@redblobgames</a> in particular uses six layers:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-noise-fields-combined.png" alt="Noise field visualization" />
  <figcaption>All six noise fields at different frequencies (1x, 2x, 4x, 16x, 32x, 64x) shown in a 3x2 grid</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-noise-triangulated.png" alt="Triangulated noise field" />
  <figcaption>Top-left corner of n<sub>2</sub> - Note that we are sampling the noises at the (centroids of the) triangles.</figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th>Layer</th>
      <th>Frequency</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>n₀</td>
      <td>1x</td>
      <td>Lowest frequency</td>
    </tr>
    <tr>
      <td>n₁</td>
      <td>2x</td>
      <td>Low frequency</td>
    </tr>
    <tr>
      <td>n₂</td>
      <td>4x</td>
      <td>Medium-low frequency</td>
    </tr>
    <tr>
      <td>n₄</td>
      <td>16x</td>
      <td>Medium-high frequency</td>
    </tr>
    <tr>
      <td>n₅</td>
      <td>32x</td>
      <td>High frequency</td>
    </tr>
    <tr>
      <td>n₆</td>
      <td>64x</td>
      <td>Highest frequency</td>
    </tr>
  </tbody>
</table>

<p>Notice the gap in numbering (n₃ is missing). This would correspond to frequency 8x, which we don’t use.</p>

<h3 id="coastal-noise-enhancement">Coastal Noise Enhancement</h3>

<p>mapgen4 starts with coastal noise enhancement. This provides control over the variation at coastlines while keeping inland elevation unaffected:</p>

\[e = \text{Paint map from Part I}\]

\[e_{coast} = e + \alpha \cdot (1 - e^4) \cdot \left(n_4 + \frac{n_5}{2} + \frac{n_6}{4}\right)\]

<p>The term \((1 - e^4)\) creates a bell curve that peaks at \(e=0\) (coastline) and decreases rapidly for \(\lvert e \rvert &gt; 0\). This modulates an fBm-like combination of our three highest frequency noise layers.</p>

<p>What matters here isn’t the exact formula or amplitudes, but the core principle: applying high-frequency detail specifically where land meets water.</p>

\[e_{tmp} = \begin{cases}
e &amp; \text{if } e_{coast} &gt; 0 \\
e_{coast} &amp; \text{if } e_{coast} \leq 0
\end{cases}\]

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-noisy-coastlines-variation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>e<sub>tmp</sub> with varying α - Showing its effect on coastline and seabed complexity</figcaption>
</figure>

<h2 id="mountain-distance-field">Mountain Distance Field</h2>

<p>Mountains need special pre-processing. If you remember from <a href="/2025/09/07/procedural-island-generation-i.html">Part I</a> in the swarm of seed points we tagged some as mountain peaks. Here we will pre-compute a <em>distance field</em> from every regular seed point to the closest mountain peak point.</p>

<p>We compute distance through the mesh topology of the Delaunay triangulation using BFS (breadth-first search). <em>i.e.,</em> we don’t use Euclidean distance. This creates more organic mountain shapes that follow the terrain’s natural connectivity.</p>

<p>The algorithm spreads outward from mountain peaks:</p>
<ol>
  <li>Start at triangles containing mountain seed points (distance = 0)</li>
  <li>Visit neighboring triangles, incrementing distance by a randomized amount</li>
  <li>The randomization creates natural ridge patterns instead of perfect cones</li>
</ol>

<p>Here’s the magic formula used for distance increment in each step:</p>

\[\Delta = s \cdot (1 + j \cdot r)\]

<p>Where:</p>
<ul>
  <li>\(s\) = spacing between triangles (uses configured Poisson disk separation)</li>
  <li>\(j\) = jaggedness parameter (0 = true topological distance, 1 = very irregular)</li>
  <li>\(r \in [-1,1]\) = random factor using triangular distribution</li>
</ul>

<p>The triangular distribution <code class="language-plaintext highlighter-rouge">rand() - rand()</code> clusters values near zero while allowing occasional larger variations. This looks more natural than uniform randomness.</p>

<p>I implemented <a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle">Fisher-Yates shuffling</a> when visiting neighbor triangles. Instead of processing neighbors in a fixed order (which would create directional bias), the order is randomly shuffled each time. This ensures mountain ridges branch out organically in all directions rather than following predictable patterns.</p>

<p>After computing distances this way, we normalize them (by the max dist, for example):</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-distance-field-jaggedness.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>d<sub>m</sub> - Normalized mountain distance field with varying jaggedness parameter</figcaption>
</figure>

<h2 id="elevation-blending">Elevation Blending</h2>

<p>The final elevation combines all components through weighted blending:</p>

\[e_{final} = \begin{cases}
\text{lerp}(e_{hill}, e_{mountain}, e_{coast}^2) &amp; \text{if } e_{coast} &gt; 0 \\
e_{coast} \cdot (\rho + n_1) &amp; \text{if } e_{coast} \leq 0
\end{cases}\]

<p>Where:</p>

<ul>
  <li>\(e_{hill} = h \cdot \bigl(1 + \text{lerp}(n_2, n_4, \tfrac{1 + n_0}{2})\bigr)\) =&gt; hill elevation with noise-modulated height</li>
  <li>\(e_{mountain} = 1 - \frac{\mu}{2^\sigma} \cdot d_m\) =&gt; mountain elevation from distance field</li>
</ul>

<p>The quadratic blend weight produces smooth transitions from hills near the coast through mixed terrain at mid-elevations to pure mountains at peaks.</p>

<p>With (editable) parameters:</p>

<ul>
  <li>\(\alpha\): Coastal noise strength (0.01)</li>
  <li>\(h\): Hill height scale (0.02)</li>
  <li>\(\rho\): Ocean depth multiplier (1.5)</li>
  <li>\(\mu\): Mountain slope (17.6)</li>
  <li>\(\sigma\): Mountain sharpness (9.8)</li>
</ul>

<h3 id="interactive-parameter-exploration">Interactive Parameter Exploration</h3>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-sharpness-variation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying mountain sharpness σ</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-mountain-jaggedness-elevation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying mountain jaggedness j</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-hill-height-variation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying hill height scale h</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-iii/chemaguerra-ocean-depth-variation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Terrain elevation with varying ocean depth multiplier ρ</figcaption>
</figure>

<h2 id="region-vs-triangle-elevation">Region (vs. Triangle) Elevation</h2>

<p>So far we’ve computed elevation for triangles. But our Voronoi regions (from Part I) also need elevations for certain stages in the rest of the series.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-triangle-region-full-animation.gif" alt="Triangle vs Region elevation animation" />
  <figcaption>Animation comparing triangle elevation vs region elevation (1 second each)</figcaption>
</figure>

<p>Each seed point defines a Voronoi region and serves as a vertex in multiple Delaunay triangles. To assign elevation to a Voronoi region, we average the elevations of all triangles that share its seed point as a vertex.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-iii/chemaguerra-triangle-region-animation.gif" alt="Triangle vs Region elevation animation" style="width: 512px; height: 512px;" />
  <figcaption>Central detail animating between triangle elevation and region elevation (1 second each)</figcaption>
</figure>

<h2 id="next-steps">Next Steps</h2>

<p>With elevation complete, our island has shape but lacks the defining features carved by water. Part IV will simulate the hydrological cycle: rainfall patterns influenced by topography, rivers flowing from peaks to ocean, and valleys carved by erosion.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://www.redblobgames.com/maps/terrain-from-noise/">Terrain from Noise</a> - Amit Patel’s Red Blob Games guide to layering noise for terrain</li>
  <li><a href="https://www.redblobgames.com/x/1843-planet-generation/">Polygonal Map Generation</a> - Red Blob Games on Voronoi-based terrain (mapgen4 inspiration)</li>
  <li><a href="https://www.redblobgames.com/x/1723-procedural-river-growing/">Distance Fields for Terrain</a> - Red Blob Games on using distance fields in terrain generation</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="elevation" /><category term="noise" /><category term="fBm" /><summary type="html"><![CDATA[Resulting terrain elevation with multi-scale noise layers and mountain peaks]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iii.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-iii.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Procedural Island Generation (II)</title><link href="http://0.0.0.0:4000/2025/09/10/procedural-island-generation-ii.html" rel="alternate" type="text/html" title="Procedural Island Generation (II)" /><published>2025-09-10T00:00:00+02:00</published><updated>2025-09-10T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/10/procedural-island-generation-ii</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/10/procedural-island-generation-ii.html"><![CDATA[<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-paintmap-hero-pixelated.png" alt="Paint map terrain elevation" />
  <figcaption>128x128 paint map defining the island's base elevation structure</figcaption>
</figure>

<p>This post is a direct continuation to <a href="/2025/09/07/procedural-island-generation-i.html">Part I</a>, where we established the terrain mesh foundation using Poisson disk sampling and Delaunay/Voronoi structures. Now we’ll breathe life into that skeletal framework by assigning elevations to create realistic terrain.</p>

<p>But how do we go from a flat mesh to believable mountains, valleys, and coastlines? The answer lies in carefully layered noise functions, distance fields, and a clever paint map system that ties everything together.</p>

<h2 id="elevation-color-gradients">Elevation Color Gradients</h2>

<p>Before we delve into elevation itself, we will pick a color gradient to map height values to natural-looking terrain colors. For example, <a href="https://github.com/redblobgames/mapgen4">mapgen4</a> (which inspired this series) uses this color gradient:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-gradient-elevation-classic.png" alt="Classic elevation gradient" />
  <figcaption>Palette used by mapgen4 - Note the double keypoint around 0.5 to differentiate water/ground</figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th>Threshold</th>
      <th>Color (Hex)</th>
      <th>Terrain Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">0.0</code></td>
      <td><code class="language-plaintext highlighter-rouge">#1A4D99</code></td>
      <td>Deep ocean</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">0.495</code></td>
      <td><code class="language-plaintext highlighter-rouge">#80B3E6</code></td>
      <td>Shallow water</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">0.5</code></td>
      <td><code class="language-plaintext highlighter-rouge">#E6E6E6</code></td>
      <td>Beach sand</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">0.505</code></td>
      <td><code class="language-plaintext highlighter-rouge">#80CC66</code></td>
      <td>Lowland grass</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">1.0</code></td>
      <td><code class="language-plaintext highlighter-rouge">#FFFFFF</code></td>
      <td>Snow peaks</td>
    </tr>
  </tbody>
</table>

<p>I will be using this more military-looking palette, though.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-gradient-elevation-muted.png" alt="Muted elevation gradient" />
  <figcaption>Military-style palette with muted colors for terrain visualization</figcaption>
</figure>

<p>For proper color blending, the elevation gradients interpolate in linear space rather than directly in sRGB, ensuring smooth transitions without washed out intermediate colors. Not that it matters that much in this particular case, though.</p>

<p>These elevation gradients are used consistently across all elevation-based visualizations in this series: paint maps, triangle elevation, region elevation, and carved terrain. Compared to raw grayscale heightmaps, they provide better visual separation between land and ocean, making the island’s boundaries and topography immediately clear.</p>

<h2 id="the-paint-map-foundation">The Paint Map Foundation</h2>

<p>Let’s start with the foundation: a low resolution <em>paint map</em> that defines the overall island shape. Think of this as the artist’s initial sketch before adding detail. The term “paint map” comes from mapgen4’s interactive feature where users can literally paint elevation changes to sculpt valleys, mountains, and other terrain features.</p>

<p>Whether or not interactive painting is supported, the paint map needs to be initialized with a procedurally generated island shape. This initialization process is our focus in this post.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-final-paint-map.png" alt="Paint map showing island shape" />
  <figcaption>Initial paint map with procedurally generated island shape</figcaption>
</figure>

<p>The range of the paint map is remapped to use the color gradient presented above:</p>

<ul>
  <li><strong>-1</strong>: Deep ocean (dark blue)</li>
  <li><strong>0</strong>: Sea level (the coastline)</li>
  <li><strong>+1</strong>: Mountain peaks (white)</li>
</ul>

<p><a href="https://github.com/redblobgames/mapgen4">@redblobgames</a> uses a 128x128 grid in mapgen4, but the actual resolution is not relevant. The idea is to have a coarse approximation to what the island elevation will look like all around. For continent-sized terrains for example you might want to use a larger grid and many more seed points. But for islands like the ones covered in this series, 128x128 will do.</p>

<h3 id="base-terrain-generation">Base Terrain Generation</h3>

<p>The foundation of our terrain starts with just two components: a distance-based shape, and fractal Brownian motion (fBm) for organic variation.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-distance-field.png" alt="Distance field" />
  <figcaption>Distance field (d) - Chebyshev distance from center, creating a square falloff</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-fbm-noise.png" alt="fBm noise" />
  <figcaption>Fractal Brownian motion (fBm) - Simplex noise (octaves=5, persistence=0.5) for organic terrain variation</figcaption>
</figure>

<p>These two components are combined according to this base elevation formula:</p>

\[e = \frac{k_{fBm} \cdot \text{fBm} + k_d \cdot (0.75 - 2 \cdot d^2)}{2}\]

<p>This formula creates a pyramid shape (highest at center, lowest at edges) and then adds noise to roughen its surface. Values above zero become land, while values below zero become water.</p>

<p>The specific coefficients and noise functions aren’t critical. Any combination of a distance field and noise will produce island-resembling terrain.</p>

<h3 id="interactive-parameter-exploration">Interactive Parameter Exploration</h3>

<p>To understand how the base parameters affect terrain generation, here are animations varying each component:</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-ii/chemaguerra-animation_fbm_amplitude.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>k_fBm varying from 0 to 1 - Controls the influence of fractal noise</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-ii/chemaguerra-animation_island_influence.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>k_d varying from 0 to 1 - Controls the influence of the distance falloff</figcaption>
</figure>

<p>\(k_{fBm} = 1.0\) and \(k_d = 0.7\) are reasonable defaults.</p>

<h2 id="mountain-ridges">Mountain Ridges</h2>

<p>While distance and fBm create the base terrain, mountain ridges add dramatic peaks and valleys to land areas.</p>

<h3 id="ridge-pattern-decomposition">Ridge Pattern Decomposition</h3>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-ridge-pattern.png" alt="Ridge pattern" />
  <figcaption>Ridge pattern (r) - The underlying ridge noise pattern computed globally</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-land-mask.png" alt="Land mask" />
  <figcaption>Land mask (e&gt;0) - Binary mask distinguishing land (white) from water (black)</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-land-strength.png" alt="Land strength" />
  <figcaption>Land strength (s) - Elevation-based multiplier (0 for water, up to 1 for peaks)</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-mountain-ridges.png" alt="Mountain ridges result" />
  <figcaption>Mountain ridges (m) - Final result: land strength × ridge pattern</figcaption>
</figure>

<p>The ridge system in mapgen4 works as follows:</p>

\[R = (\text{noise}_1 + \text{noise}_2) \in [-2, 2]\]

\[r = \text{saturate}(0, 1 - \lvert R \rvert)\]

\[s = \text{saturate}(5e)\]

\[r_{final} = \text{clamp}(s \cdot r, e, 3e)\]

<p>The ridge pattern combines two octaves from differently-seeded noise functions. The \(1 - \lvert R \rvert\) transformation creates the characteristic tubular appearance, although any similar function that emphasizes values near zero would work equally well.</p>

<p>The strength multiplier \(s\) ensures ridges fade naturally at coastlines and intensify at higher elevations, confining mountain formation to land areas while scaling with terrain height.</p>

<h3 id="ridge-influence-animation">Ridge Influence Animation</h3>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-ii/chemaguerra-animation_ridge_influence.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Ridge influence varying from 0 to 1: controls how strongly ridges modify the base terrain</figcaption>
</figure>

<h2 id="final-composite">Final Composite</h2>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-final-paint-map.png" alt="Final composite" />
  <figcaption>128x128 paint map - raw visualization</figcaption>
</figure>

\[e_{final} = e + r_{final}\]

<p>Although the paint map operates at low resolution (128x128), the implementation supports sampling at any floating-point coordinate within the unit square through bilinear interpolation, providing smooth transitions between grid points.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-ii/chemaguerra-final-paint-map-512.png" alt="Final bilinear sampled" />
  <figcaption>128x128 paint map - bilinearly sampled in the unit square domain</figcaption>
</figure>

<h2 id="next-steps">Next Steps</h2>

<p>We’ve established the paint map foundation with fBm noise, island shape constraints, and mountain ridge patterns. This 128x128 grid provides the broad strokes of our terrain. Part III will add the fine details: multi-scale noise layers, distance-based mountain peaks, and elevation blending that brings the terrain to life.</p>

<h2 id="valuable-resources">Valuable Resources</h2>

<ul>
  <li><a href="https://thebookofshaders.com/13/">Fractal Brownian Motion</a> - The Book of Shaders fBm tutorial</li>
  <li><a href="https://www.redblobgames.com/maps/terrain-from-noise/">Terrain from Noise</a> - Red Blob Games elevation techniques</li>
  <li><a href="https://www.redblobgames.com/maps/terrain-from-noise/#elevation">Amit Patel’s Map Generation</a> - Paint map concepts</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="elevation" /><category term="noise" /><category term="computationalgeometry" /><category term="fBm" /><summary type="html"><![CDATA[128x128 paint map defining the island's base elevation structure]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-ii.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-ii.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Procedural Island Generation (I)</title><link href="http://0.0.0.0:4000/2025/09/07/procedural-island-generation-i.html" rel="alternate" type="text/html" title="Procedural Island Generation (I)" /><published>2025-09-07T00:00:00+02:00</published><updated>2025-09-07T00:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/09/07/procedural-island-generation-i</id><content type="html" xml:base="http://0.0.0.0:4000/2025/09/07/procedural-island-generation-i.html"><![CDATA[<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-procedural-island-generation-terrain-mesh.png" alt="Terrain mesh visualization" />
  <figcaption>Procedurally generated terrain mesh with organic triangulation</figcaption>
</figure>

<p>This post marks the beginning of a series on procedural island generation. I recently fell in love with <a href="https://twitter.com/redblobgames">Amit Patel’s</a> brilliant <a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">polygon map generation</a> articles, and this series will be my own exploration and interpretation of those techniques.</p>

<p>This series assumes familiarity with computational geometry and procedural generation concepts such as Delaunay triangulation, Voronoi diagrams, fBm, etc…</p>

<p>To challenge myself (I’m usually a C/C++ person), I decided to implement this in Rust. But I will barely delve into implementation details though. This series will focus on the algorithm itself, with neat (hopefully!) visualizations of the intermediate calculations.</p>

<h2 id="motivation">Motivation</h2>

<p>The main reason why I am tinkering with terrain generation now is <strong>Strike Back</strong>, the <a href="https://x.com/topotoylabs/status/1750315804170612745">flight sim I’ve been developing on and off</a>. I will be using this algorithm as a foundation to create battle arenas which I plan to extend eventually.</p>

<h3 id="working-domain">Working Domain</h3>

<p>Procedural terrain generation offers infinite variety from finite code. But islands make particularly interesting subjects because they’re self-contained worlds with natural boundaries. The polygon-based approach we’ll explore provides a nice balance between organic appearance and computational tractability.</p>

<p>We’ll work in a <em>regular grid</em> within a <em>unit square domain</em>. While regular grids are the classic approach for terrain generation (much simpler implementation, no topological headaches), irregular grids produce far more organic-looking geometry. That’s what we’re after.</p>

<p>Irregular grids have been having a moment lately, BTW. Especially since <a href="https://store.steampowered.com/app/1291340/Townscaper/">Townscaper</a> captured everyone’s imagination.</p>

<h2 id="seed-point-generation">Seed Point Generation</h2>

<p>The foundation of an organic-looking terrain mesh is the distribution of its vertices. Random point placement leads to ugly, irregular triangulation. We need something better. Ideally, we need points that are random yet evenly spaced. This is where blue noise comes in.</p>

<h3 id="blue-noise-and-poisson-disk-sampling">Blue Noise and Poisson Disk Sampling</h3>

<p>Blue noise distributions have a specific property: points maintain a minimum distance from each other while filling space uniformly. No clumping, no voids. The spectrum of blue noise is weighted toward higher frequencies, which translates visually to pleasant, nature-resembling distributions.</p>

<p>Bridson’s algorithm for Poisson disk sampling is elegantly simple:</p>

<ol>
  <li>Start with a random initial point</li>
  <li>Generate candidate points in an annulus around existing points</li>
  <li>Accept candidates that are far enough from all existing points</li>
  <li>Repeat until no valid candidates remain</li>
</ol>

<p>The key parameters are:</p>
<ul>
  <li>\(r\): minimum distance acceptable between points</li>
  <li>\(k\): number of candidates to try before rejection (typically 30)</li>
</ul>

<p>The algorithm maintains an “active list” of points that might still have valid neighbors. When a point fails to produce valid candidates after \(k\) attempts, it is removed from the active list.</p>

<p>Spatial indexing is crucial for efficient nearest-neighbor queries in Bridson’s algorithm. A simple grid works well: divide space into cells of size \(r/\sqrt{2}\), ensuring at most one point per cell for \(O(1)\) lookups.</p>

<p>Below is an auxiliary visualization animation I wrote in Python:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-bridson-algorithm.gif" alt="Bridson's algorithm in action" />
  <figcaption>Bridson's algorithm generating Poisson disk distribution with minimum distance constraint</figcaption>
</figure>

<h3 id="lloyds-relaxation">Lloyd’s Relaxation</h3>

<p>There are different families of algorithms for blue noise generation. Dart-throwing methods, void-and-cluster, Fourier-based methods, relaxation methods, …</p>

<p>While we will stick to Bridson’s algorithm as described above, I am fond of Lloyd’s relaxation (also known as Voronoi iteration). And because we will be working with the Voronoi diagram later I thought it would be worth mentioning.</p>

<p>The process is beautifully simple:</p>
<ol>
  <li>Start with any set of random points.</li>
  <li>Compute the Voronoi diagram for current points</li>
  <li>Move each point to the centroid of its Voronoi cell</li>
  <li>Repeat until convergence</li>
</ol>

<p>This iterative process naturally evolves toward a hexagonal-dominant grid, the most efficient packing in 2D.</p>

<p>Below is another animation I made in Python:</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-lloyd-relaxation.gif" alt="Lloyd's relaxation with Voronoi cells visible" />
  <figcaption>Lloyd's relaxation iteratively moving points to Voronoi cell centroids</figcaption>
</figure>

<p>Lloyd’s relaxation is computationally expensive and produces overly regular distributions that can look artificial for terrain. Bridson strikes a better balance: it’s fast and maintains the organic irregularity we want.</p>

<h2 id="boundary-treatment">Boundary Treatment</h2>

<p>Instead of letting Poisson disk sampling run across the entire domain, I restrict it to a shrunk area, leaving a gap of radius \(r\) around the edges. Then I add two rings of evenly-spaced points: one at the boundary itself (green), and another ring \(r\) units outside the domain (red).</p>

<p>This approach ensures full coverage with well-behaved triangles at the edges for Delaunay triangulation and simplifies handling of unbounded Voronoi regions.</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-bridson-separation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Boundary treatment with interior points (white), boundary ring (green), and exterior ring (red)</figcaption>
</figure>

<p>In the animation above, working with a unit square domain and \(r = \frac{1}{200}\), we’d theoretically fit up to \(\left(\frac{1}{r}\right)^2 = 200^2 = 40000\) points in a perfect grid. But Poisson disk sampling produces a looser packing. With the annulus radius range \([r, 2r]\) and rejection sampling, we get ~27000 points in practice, about 68% of the theoretical maximum.</p>

<h2 id="delaunay-triangulation">Delaunay Triangulation</h2>

<p>With our well-distributed seed points, we need to create a mesh. Delaunay triangulation is the go-to choice for terrain generation because it maximizes the minimum angle of all triangles, avoiding skinny, degenerate triangles that cause numerical and visual issues.</p>

<p>In the visualizations throughout this post, red dots will represent seed points (Delaunay vertices), while blue dots will represent Voronoi vertices (corresponding to Delaunay triangle centers).</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-delaunay-static.png" alt="Delaunay triangulation static view" />
  <figcaption>Delaunay triangulation maximizing minimum angles for well-formed triangles</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-delaunay-tile.png" alt="Delaunay triangulation detail view" />
  <figcaption>Close-up of Delaunay triangulation showing triangle quality</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-delaunay-mesh.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Animated Delaunay triangulation with varying point density</figcaption>
</figure>

<p>For the implementation, I went with Rust’s <code class="language-plaintext highlighter-rouge">delaunator</code> crate. It’s a robust, battle-tested port of the JavaScript library of the same name, implementing a fast sweep-line algorithm that runs in \(O(n \log n)\) time. The cyclic animation is just the same code but decreasing/increasing \(r\) (more/fewer points).</p>

<h2 id="voronoi-diagram">Voronoi Diagram</h2>

<p>The Voronoi diagram is the dual of the Delaunay triangulation. Each Delaunay triangle vertex becomes a Voronoi cell, and each Delaunay edge corresponds to a Voronoi edge. For terrain generation, Voronoi cells are a staple: they define convex-shaped regions around each seed point, perfect for assigning terrain properties like elevation, moisture, and biome types.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-voronoi-full.png" alt="Voronoi cells full view" />
  <figcaption>Voronoi diagram showing cell boundaries and seed points</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-voronoi-circumcenters-tile.png" alt="Voronoi with circumcenters detail view" />
  <figcaption>Voronoi cells using triangle circumcenters as vertices</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-voronoi-cells.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Animated Voronoi diagram with changing cell structure</figcaption>
</figure>

<p>Rather than using an off-the-shelf solution, I built the dual construction using my topology library <strong>TopoKit</strong>, recently ported to Rust. The half-edge (DCEL) data structure I have there makes it trivial to traverse neighbors, and maintain the topological relationships we need for terrain generation. TopoKit also handles serialization, relaxation, LOD subdivision, and other features I’ll tap into later.</p>

<h2 id="circumcenters-vs-centroids">Circumcenters vs. Centroids</h2>

<p>The classical Voronoi diagram uses triangle circumcenters as Voronoi vertices. The circumcenter is equidistant from all three triangle vertices, making it the natural meeting point of three Voronoi cells. However the circumcenter potentially lies outside its triangle if it is obtuse enough. For very flat triangles near boundaries, circumcenters can be far from their triangles, sometimes even outside the domain entirely.</p>

<p>A clever trick I learned from <a href="https://twitter.com/redblobgames">@redblobgames</a> is using triangle centroids instead of circumcenters for Voronoi vertices. This simple change produces smoother, more bubble-like regions with gentler transitions between cells. These softer angles are ideal for terrain generation, and the computation is actually simpler.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-voronoi-centroids.png" alt="Voronoi diagram using centroids" />
  <figcaption>Voronoi cells using triangle centroids for smoother, bubble-like regions</figcaption>
</figure>

<p>Here’s the main takeaway (an animated comparison):</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-voronoi-comparison.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Animated comparison: circumcenters (angular) vs centroids (smooth)</figcaption>
</figure>

<p>We will go with centroids (instead of circumcenters) for the rest of the series.</p>

<h2 id="quad-mesh-construction">Quad Mesh Construction</h2>

<p>So we have:</p>
<ul>
  <li><strong>Red vertices</strong>: Original seed points (Delaunay vertices)</li>
  <li><strong>Blue vertices</strong>: Voronoi vertices (triangle centers)</li>
</ul>

<p>Both vertex sets contribute to the final terrain mesh. Throughout this series, we’ll compute attributes like elevation for both red and blue vertices, then triangulate the combined point set to generate the final terrain geometry.</p>

<h3 id="the-red-blue-pattern">The Red-Blue Pattern</h3>

<p>A natural way to combine both the Delaunay and Voronoi meshes is to form quads by each red-blue-red-blue cycle, traversing around shared edges:</p>

<ol>
  <li>Start with a Delaunay edge connecting two red vertices</li>
  <li>This edge is dual to a Voronoi edge connecting two blue vertices</li>
  <li>The four vertices form a natural quad: red-blue-red-blue</li>
</ol>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-quads-static.png" alt="Quad mesh static view" />
  <figcaption>Quad mesh formed by red-blue-red-blue vertex cycles</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-quads-tile.png" alt="Quad mesh detail view" />
  <figcaption>Detail of quad mesh showing diamond pattern structure</figcaption>
</figure>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-quad-mesh.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Animated quad mesh construction from Delaunay-Voronoi dual</figcaption>
</figure>

<p>In the resulting quad mesh:</p>
<ul>
  <li>Red vertices have high valence (many incident edges)</li>
  <li>Blue vertices typically have valence 3 (from triangle origins)</li>
  <li>Edge flow follows a characteristic diamond pattern</li>
</ul>

<h2 id="mountain-peak-seed-points">Mountain Peak Seed Points</h2>

<p>To generate realistic terrain with well-placed mountain peaks, we must flag a special subset of seed points that will become elevation maxima. This two-scale approach ensures mountains maintain natural spacing while aligning perfectly with our existing mesh vertices.</p>

<p>The process leverages a second Bridson’s algorithm run with a much larger radius, typically 4-16 times the original \(r\). This coarse distribution determines where mountain peaks should roughly appear, ensuring they maintain natural spacing across the terrain.</p>

<p>Here’s the algorithm:</p>
<ol>
  <li>Build a kD-tree from the fine-grain seed points for efficient nearest-neighbor queries</li>
  <li>Run Bridson’s algorithm with radius \(R \gg r\) to get coarse peak candidates</li>
  <li>For each coarse candidate, find its nearest neighbor in the fine distribution</li>
  <li>Flag these nearest neighbors as mountain peak seeds</li>
</ol>

<p>Spatial hashing (the kD-tree in my case) is crucial here for performance. With thousands of seed points, nearest-neighbor search would be prohibitively slow otherwise.</p>

<p>This approach has several advantages:</p>
<ul>
  <li>Natural spacing: Mountains won’t cluster unnaturally or leave large empty regions</li>
  <li>Mesh alignment: Peak points are actual mesh vertices, not interpolated positions</li>
  <li>Controllable density: Adjusting \(R\) directly controls mountain frequency</li>
  <li>Reproducible: Given the same random seed, peak placement is deterministic</li>
</ul>

<p>These flagged mountain peaks will later serve as local maxima during elevation assignment, with height falloff based on distance. But that’s a topic for Part II.</p>

<figure>
  <video autoplay="" loop="" muted="" playsinline="" style="width: 512px; height: 512px;">
    <source src="/uploads/2025/procedural-island-generation-i/chemaguerra-mountain-peak-separation.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Mountain peak distribution with varying separation radius R</figcaption>
</figure>

<p>Above is an animation with varying mountain separation \(R\) over the same set of regular seed points.</p>

<h2 id="next-steps">Next Steps</h2>

<p>This is the type of distribution (~27000 points) we will be using throughout the rest of the series.</p>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-mountain-seed-points.png" alt="Mountain peak seed points visualization" />
  <figcaption>Final distribution of ~27,000 seed points for terrain generation</figcaption>
</figure>

<figure>
  <img src="/uploads/2025/procedural-island-generation-i/chemaguerra-seed-points-tile.png" alt="Mountain peak seed points detail view" />
  <figcaption>Detail view showing organic point distribution quality</figcaption>
</figure>

<p>With our mesh structures in place we have the skeletal framework for our island. Part II will dive into elevation assignment using noise functions, creating realistic height maps that produce convincing mountains, valleys, and coastlines.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">Amit Patel’s Polygon Map Generation</a> - The inspiration for this series</li>
  <li><a href="https://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-poissondisk.pdf">Bridson’s Original Paper</a> - Fast Poisson disk sampling</li>
  <li><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s Algorithm</a> - The mathematical foundation</li>
</ul>]]></content><author><name>Chema Guerra</name></author><category term="graphics" /><category term="procedural" /><category term="procgen" /><category term="procedural" /><category term="procgen" /><category term="terrain" /><category term="voronoi" /><category term="delaunay" /><category term="computationalgeometry" /><category term="bluenoise" /><summary type="html"><![CDATA[Procedurally generated terrain mesh with organic triangulation This post marks the beginning of a series on procedural island generation. I recently fell in love with Amit Patel’s brilliant polygon map generation articles, and this series will be my own exploration and interpretation of those techniques. This series assumes familiarity with computational geometry and procedural generation concepts such as Delaunay triangulation, Voronoi diagrams, fBm, etc… To challenge myself (I’m usually a C/C++ person), I decided to implement this in Rust. But I will barely delve into implementation details though. This series will focus on the algorithm itself, with neat (hopefully!) visualizations of the intermediate calculations. Motivation The main reason why I am tinkering with terrain generation now is Strike Back, the flight sim I’ve been developing on and off. I will be using this algorithm as a foundation to create battle arenas which I plan to extend eventually. Working Domain Procedural terrain generation offers infinite variety from finite code. But islands make particularly interesting subjects because they’re self-contained worlds with natural boundaries. The polygon-based approach we’ll explore provides a nice balance between organic appearance and computational tractability. We’ll work in a regular grid within a unit square domain. While regular grids are the classic approach for terrain generation (much simpler implementation, no topological headaches), irregular grids produce far more organic-looking geometry. That’s what we’re after. Irregular grids have been having a moment lately, BTW. Especially since Townscaper captured everyone’s imagination. Seed Point Generation The foundation of an organic-looking terrain mesh is the distribution of its vertices. Random point placement leads to ugly, irregular triangulation. We need something better. Ideally, we need points that are random yet evenly spaced. This is where blue noise comes in. Blue Noise and Poisson Disk Sampling Blue noise distributions have a specific property: points maintain a minimum distance from each other while filling space uniformly. No clumping, no voids. The spectrum of blue noise is weighted toward higher frequencies, which translates visually to pleasant, nature-resembling distributions. Bridson’s algorithm for Poisson disk sampling is elegantly simple: Start with a random initial point Generate candidate points in an annulus around existing points Accept candidates that are far enough from all existing points Repeat until no valid candidates remain The key parameters are: \(r\): minimum distance acceptable between points \(k\): number of candidates to try before rejection (typically 30) The algorithm maintains an “active list” of points that might still have valid neighbors. When a point fails to produce valid candidates after \(k\) attempts, it is removed from the active list. Spatial indexing is crucial for efficient nearest-neighbor queries in Bridson’s algorithm. A simple grid works well: divide space into cells of size \(r/\sqrt{2}\), ensuring at most one point per cell for \(O(1)\) lookups. Below is an auxiliary visualization animation I wrote in Python: Bridson's algorithm generating Poisson disk distribution with minimum distance constraint Lloyd’s Relaxation There are different families of algorithms for blue noise generation. Dart-throwing methods, void-and-cluster, Fourier-based methods, relaxation methods, … While we will stick to Bridson’s algorithm as described above, I am fond of Lloyd’s relaxation (also known as Voronoi iteration). And because we will be working with the Voronoi diagram later I thought it would be worth mentioning. The process is beautifully simple: Start with any set of random points. Compute the Voronoi diagram for current points Move each point to the centroid of its Voronoi cell Repeat until convergence This iterative process naturally evolves toward a hexagonal-dominant grid, the most efficient packing in 2D. Below is another animation I made in Python: Lloyd's relaxation iteratively moving points to Voronoi cell centroids Lloyd’s relaxation is computationally expensive and produces overly regular distributions that can look artificial for terrain. Bridson strikes a better balance: it’s fast and maintains the organic irregularity we want. Boundary Treatment Instead of letting Poisson disk sampling run across the entire domain, I restrict it to a shrunk area, leaving a gap of radius \(r\) around the edges. Then I add two rings of evenly-spaced points: one at the boundary itself (green), and another ring \(r\) units outside the domain (red). This approach ensures full coverage with well-behaved triangles at the edges for Delaunay triangulation and simplifies handling of unbounded Voronoi regions. Your browser does not support the video tag. Boundary treatment with interior points (white), boundary ring (green), and exterior ring (red) In the animation above, working with a unit square domain and \(r = \frac{1}{200}\), we’d theoretically fit up to \(\left(\frac{1}{r}\right)^2 = 200^2 = 40000\) points in a perfect grid. But Poisson disk sampling produces a looser packing. With the annulus radius range \([r, 2r]\) and rejection sampling, we get ~27000 points in practice, about 68% of the theoretical maximum. Delaunay Triangulation With our well-distributed seed points, we need to create a mesh. Delaunay triangulation is the go-to choice for terrain generation because it maximizes the minimum angle of all triangles, avoiding skinny, degenerate triangles that cause numerical and visual issues. In the visualizations throughout this post, red dots will represent seed points (Delaunay vertices), while blue dots will represent Voronoi vertices (corresponding to Delaunay triangle centers). Delaunay triangulation maximizing minimum angles for well-formed triangles Close-up of Delaunay triangulation showing triangle quality Your browser does not support the video tag. Animated Delaunay triangulation with varying point density For the implementation, I went with Rust’s delaunator crate. It’s a robust, battle-tested port of the JavaScript library of the same name, implementing a fast sweep-line algorithm that runs in \(O(n \log n)\) time. The cyclic animation is just the same code but decreasing/increasing \(r\) (more/fewer points). Voronoi Diagram The Voronoi diagram is the dual of the Delaunay triangulation. Each Delaunay triangle vertex becomes a Voronoi cell, and each Delaunay edge corresponds to a Voronoi edge. For terrain generation, Voronoi cells are a staple: they define convex-shaped regions around each seed point, perfect for assigning terrain properties like elevation, moisture, and biome types. Voronoi diagram showing cell boundaries and seed points Voronoi cells using triangle circumcenters as vertices Your browser does not support the video tag. Animated Voronoi diagram with changing cell structure Rather than using an off-the-shelf solution, I built the dual construction using my topology library TopoKit, recently ported to Rust. The half-edge (DCEL) data structure I have there makes it trivial to traverse neighbors, and maintain the topological relationships we need for terrain generation. TopoKit also handles serialization, relaxation, LOD subdivision, and other features I’ll tap into later. Circumcenters vs. Centroids The classical Voronoi diagram uses triangle circumcenters as Voronoi vertices. The circumcenter is equidistant from all three triangle vertices, making it the natural meeting point of three Voronoi cells. However the circumcenter potentially lies outside its triangle if it is obtuse enough. For very flat triangles near boundaries, circumcenters can be far from their triangles, sometimes even outside the domain entirely. A clever trick I learned from @redblobgames is using triangle centroids instead of circumcenters for Voronoi vertices. This simple change produces smoother, more bubble-like regions with gentler transitions between cells. These softer angles are ideal for terrain generation, and the computation is actually simpler. Voronoi cells using triangle centroids for smoother, bubble-like regions Here’s the main takeaway (an animated comparison): Your browser does not support the video tag. Animated comparison: circumcenters (angular) vs centroids (smooth) We will go with centroids (instead of circumcenters) for the rest of the series. Quad Mesh Construction So we have: Red vertices: Original seed points (Delaunay vertices) Blue vertices: Voronoi vertices (triangle centers) Both vertex sets contribute to the final terrain mesh. Throughout this series, we’ll compute attributes like elevation for both red and blue vertices, then triangulate the combined point set to generate the final terrain geometry. The Red-Blue Pattern A natural way to combine both the Delaunay and Voronoi meshes is to form quads by each red-blue-red-blue cycle, traversing around shared edges: Start with a Delaunay edge connecting two red vertices This edge is dual to a Voronoi edge connecting two blue vertices The four vertices form a natural quad: red-blue-red-blue Quad mesh formed by red-blue-red-blue vertex cycles Detail of quad mesh showing diamond pattern structure Your browser does not support the video tag. Animated quad mesh construction from Delaunay-Voronoi dual In the resulting quad mesh: Red vertices have high valence (many incident edges) Blue vertices typically have valence 3 (from triangle origins) Edge flow follows a characteristic diamond pattern Mountain Peak Seed Points To generate realistic terrain with well-placed mountain peaks, we must flag a special subset of seed points that will become elevation maxima. This two-scale approach ensures mountains maintain natural spacing while aligning perfectly with our existing mesh vertices. The process leverages a second Bridson’s algorithm run with a much larger radius, typically 4-16 times the original \(r\). This coarse distribution determines where mountain peaks should roughly appear, ensuring they maintain natural spacing across the terrain. Here’s the algorithm: Build a kD-tree from the fine-grain seed points for efficient nearest-neighbor queries Run Bridson’s algorithm with radius \(R \gg r\) to get coarse peak candidates For each coarse candidate, find its nearest neighbor in the fine distribution Flag these nearest neighbors as mountain peak seeds Spatial hashing (the kD-tree in my case) is crucial here for performance. With thousands of seed points, nearest-neighbor search would be prohibitively slow otherwise. This approach has several advantages: Natural spacing: Mountains won’t cluster unnaturally or leave large empty regions Mesh alignment: Peak points are actual mesh vertices, not interpolated positions Controllable density: Adjusting \(R\) directly controls mountain frequency Reproducible: Given the same random seed, peak placement is deterministic These flagged mountain peaks will later serve as local maxima during elevation assignment, with height falloff based on distance. But that’s a topic for Part II. Your browser does not support the video tag. Mountain peak distribution with varying separation radius R Above is an animation with varying mountain separation \(R\) over the same set of regular seed points. Next Steps This is the type of distribution (~27000 points) we will be using throughout the rest of the series. Final distribution of ~27,000 seed points for terrain generation Detail view showing organic point distribution quality With our mesh structures in place we have the skeletal framework for our island. Part II will dive into elevation assignment using noise functions, creating realistic height maps that produce convincing mountains, valleys, and coastlines. Resources Amit Patel’s Polygon Map Generation - The inspiration for this series Bridson’s Original Paper - Fast Poisson disk sampling Lloyd’s Algorithm - The mathematical foundation]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-i.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2025/chemaguerra-procedural-island-generation-i.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Spherical Harmonics</title><link href="http://0.0.0.0:4000/2025/08/25/spherical-harmonics.html" rel="alternate" type="text/html" title="Spherical Harmonics" /><published>2025-08-25T10:00:00+02:00</published><updated>2025-08-25T10:00:00+02:00</updated><id>http://0.0.0.0:4000/2025/08/25/spherical-harmonics</id><content type="html" xml:base="http://0.0.0.0:4000/2025/08/25/spherical-harmonics.html"><![CDATA[<blockquote>
  <p><strong>Note:</strong> This post was written nearly 2 years ago and never published. I’m posting it now as-is, with some sections incomplete but the core content intact.</p>
</blockquote>

<p>This post is a quick crash course on what <a href="https://en.wikipedia.org/wiki/Spherical_harmonics">Spherical Harmonics</a> are and how they can be used to efficiently encode and decode <em>incoming radiance</em> (<em>i.e.,</em> a spherical view of the scene) at a point.</p>

<h2 id="motivation">Motivation</h2>

<h3 id="radiance-and-irradiance">Radiance and irradiance</h3>

<p>In the context of rendering:</p>

<p><strong>Radiance</strong> (\(L\)) is the amount of light that flows through or is emitted from a point in space in a particular direction.</p>

<p><strong>Irradiance</strong> (\(E\)) is the total amount of light energy incident upon a surface at a specific point.</p>

\[E=\int_{\Omega}{L(\mathbf{\omega})cos(\theta)\mathrm{d}\mathbf{\omega}}\]

<p>Where:</p>

<ul>
  <li>\(E\) is the irradiance at a point on the surface.</li>
  <li>\(L(\mathbf{\omega})\) is the radiance arriving from direction \(\mathbf{\omega}\).</li>
  <li>\(\theta\) is the angle between the surface normal and the direction \(\mathbf{\omega}\).</li>
  <li>\(d\mathbf{\omega}\) represents the differential solid angle in the hemisphere of incoming light directions.</li>
</ul>

<h3 id="light-probes">Light probes</h3>

<p>The definition above talks about <em>surfaces</em> (with a normal). That is, radiance arriving at the point from the positive side of the surface. However, in this post we are interested on the radiance arriving at a point from all directions:</p>

\[E=\int_{O}{L(\mathbf{\omega})\mathrm{d}\mathbf{\omega}}\]

<p>Under certain circumstances we may want to precompute-and-store the incoming radiance \(L\) at a point or a collection of points in the scene in order to approximate illumination dynamically in real-time (possibly making use of \(E\) somehow).</p>

<p>\(L\) is a function defined on the sphere, and is called light probe…</p>

<p>\(E\) shows up in the <a href="https://en.wikipedia.org/wiki/Rendering_equation">Rendering Equation</a> but again, in this post we’re only interested in an efficient way to precompute-and-store \(L(\mathbf{\omega})\).</p>

<p>In a real scenario, light probes can be placed either manually or automatically and they are meant to suffciently cover the scene. The goal of each individual light probe is to keep track of what illumination looks like in the volumetric neighborhood of the point they are centered at. Typically, nearby light probes are sampled and the samples obtained are interpolated to figure out the illumination arriving at any given 3D point. This can give a decent and fast-to-query approximation to Global Illumination and/or soft shadowing.</p>

<p>The information stored by a light probe can be pictured of as a panoramic view of what the point sees around itself. So typical ways to initialize (or update) a light probe are to literally render/rasterize the scene around (maybe with 6 cubemap-arranged cameras) or ray-tracing the surroundings stochastically.</p>

<p>Whichever the method, the resulting irradiance is a spherical collection of colors-and-their-magnitudes. Or, in better words, a function defined on the sphere which yields a color when evaluated at each unit direction. Which, in more humane terms, is a \(360\deg\) photograph of the scene from the point (a panorama or a cubemap if you prefer).</p>

<p>Those are usually rather heavy, and since scenes typically require (many) thousands of light probes to sufficiently cover their volume, finding a highly efficient and performant (albeit lossless) way to encode such spherical color maps sounds ideal.</p>

<p>It is important to note that such radiances-arriving-at-a-point maps can be very high-frequency as soon as we involve high-intensity point lights (such as the sun) and hard shadows. So in some engines all direct lighting coming from hard lights is calculated dynamically and light probes are used exclusively for indirect illumination and soft shadowing, which is much lower frequency.</p>

<p>So let’s establish the goal of this post as:</p>

<blockquote>
  <p>Find a highly efficient/performant way to encode (low-frequency) spherical color maps with as little loss as possible.</p>
</blockquote>

<h3 id="cubemaps">Cubemaps</h3>

<p>Throughout this post I will display \(L(\mathbf{\omega})\) as cubemaps because cubemap projections exhibit little distortion compared to angular or latitude-longitude maps.</p>

<p>It is important to note that in doing so, the integrals we will be calculating below need to account for distortion in their differential element.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-differential.png" alt="Directions and differentials" /></p>

<p>The rgb-coded cubemap represents all the unit directions in a sphere \(\mathbf{\omega}\). The grayscale cubemap represents the differential element conversion between the unit sphere and the unit cubemap \(\mathrm{d}\mathbf{\omega}\). The HDR map unfolded as a cubemap is an example of an incoming radiance map \(L(\mathbf{\omega})\).</p>

<p>Sanity check: The sum of all the grayscale patches must be \(4\pi\) = the surface of the unit sphere.</p>

<h2 id="spherical-harmonics">Spherical Harmonics</h2>

<p>SHs have no particular connection with rendering or radiance encoding. They are a generic mathematical contraption with which you can <em>project</em> some input data given in an input realm (usually space) onto another realm (usually frequency-related). This projection process is also reversible, which allows for encoding/decoding.</p>

<p>This is all quite reminiscent of the Fourier Transform &amp; Co..</p>

<p>SH are particularly interesting in the context of Computer Graphics because their domain is the unit sphere, which makes them ideal to encode <em>directional information</em>, such as light probes as described above.</p>

<h3 id="orthogonality-and-key-properties">Orthogonality and Key Properties</h3>

<p>Spherical Harmonics have several mathematical properties that make them ideal for encoding spherical functions:</p>

<p><strong>Orthogonality:</strong> The SH functions are orthogonal over the sphere. This property allows us to decompose any spherical function into independent components without interference between different harmonics.</p>

<p><strong>Completeness:</strong> Any square-integrable function on the sphere can be represented as a linear combination of spherical harmonics. For rendering, this means we can approximate any incoming radiance distribution to arbitrary precision by using enough SH coefficients.</p>

<p><strong>Rotation Invariance:</strong> The total power in each frequency band (each level \(l\)) is preserved under rotation. This makes SH coefficients stable when objects or light probes are rotated.</p>

<p><strong>Frequency Separation:</strong> Lower-order harmonics capture low-frequency (smooth) variations, while higher-order harmonics capture high-frequency (detailed) variations. This natural frequency ordering makes SH perfect for level-of-detail approximations.</p>

<p>From Wikipedia:</p>

<p><em>“Since the Spherical Harmonics form a complete set of orthogonal functions and thus an orthonormal basis, each function defined on the surface of a sphere can be written as a sum of these spherical harmonics.”</em></p>

<h3 id="what-the-shs-look-like">What the SHs look like</h3>

<p>Laplace’s Spherical Harmonics are denoted \(Y_l^m(\omega)\) where:</p>

<ul>
  <li>\(l\) is called the <strong>degree</strong> or <strong>order</strong> (ranging from 0 to infinity in theory, but we truncate for practical use).</li>
  <li>\(m\) is called the <strong>order within degree</strong> (ranging from \(-l\) to \(+l\)).</li>
</ul>

<p>For a given order \(l\), there are \(2l+1\) different harmonics, each identified by a different value of \(m\). This gives us:</p>
<ul>
  <li>Level 0 (\(l=0\)): 1 harmonic</li>
  <li>Level 1 (\(l=1\)): 3 harmonics</li>
  <li>Level 2 (\(l=2\)): 5 harmonics</li>
  <li>Level 3 (\(l=3\)): 7 harmonics</li>
  <li>Total for \(L\) levels: \(L^2\) harmonics</li>
</ul>

<p>The harmonics are essentially different “shapes” or “patterns” on the sphere that form an orthogonal basis. Below is my implementation for the first 4 levels</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k01</span> <span class="o">=</span> <span class="mf">0.2820947918</span><span class="p">;</span>  <span class="c1">// sqrt(  1/pi)/2.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k02</span> <span class="o">=</span> <span class="mf">0.4886025119</span><span class="p">;</span>  <span class="c1">// sqrt(  3/pi)/2.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k03</span> <span class="o">=</span> <span class="mf">1.0925484306</span><span class="p">;</span>  <span class="c1">// sqrt( 15/pi)/2.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k04</span> <span class="o">=</span> <span class="mf">0.3153915652</span><span class="p">;</span>  <span class="c1">// sqrt(  5/pi)/4.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k05</span> <span class="o">=</span> <span class="mf">0.5462742153</span><span class="p">;</span>  <span class="c1">// sqrt( 15/pi)/4.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k06</span> <span class="o">=</span> <span class="mf">0.5900435860</span><span class="p">;</span>  <span class="c1">// sqrt( 70/pi)/8.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k07</span> <span class="o">=</span> <span class="mf">2.8906114210</span><span class="p">;</span>  <span class="c1">// sqrt(105/pi)/2.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k08</span> <span class="o">=</span> <span class="mf">0.4570214810</span><span class="p">;</span>  <span class="c1">// sqrt( 42/pi)/8.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k09</span> <span class="o">=</span> <span class="mf">0.3731763300</span><span class="p">;</span>  <span class="c1">// sqrt(  7/pi)/4.</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="n">T</span> <span class="n">m_k10</span> <span class="o">=</span> <span class="mf">1.4453057110</span><span class="p">;</span>  <span class="c1">// sqrt(105/pi)/4.</span>

<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_0_0</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span>   <span class="p">)</span> <span class="p">{</span> <span class="k">return</span>    <span class="n">m_k01</span><span class="p">;</span> <span class="p">}</span>

<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_1_0</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k02</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_1_1</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k02</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_1_2</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k02</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">);</span> <span class="p">}</span>

<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_2_0</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k03</span> <span class="o">*</span>               <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span>                         <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_2_1</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k03</span> <span class="o">*</span>               <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span>                         <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_2_2</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k04</span> <span class="o">*</span>       <span class="p">(</span> <span class="p">(</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="p">)</span> <span class="o">-</span>   <span class="mi">1</span>               <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_2_3</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k03</span> <span class="o">*</span>               <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span>                         <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_2_4</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k05</span> <span class="o">*</span>       <span class="p">(</span> <span class="p">(</span>     <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span>     <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>

<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_0</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k06</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span>     <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_1</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k07</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span>     <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">)</span>                     <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_2</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k08</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="mi">1</span>             <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_3</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k09</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="mi">3</span>             <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_4</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k08</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="mi">1</span>             <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_5</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span>  <span class="n">m_k10</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span>     <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span>     <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
<span class="k">static</span> <span class="n">T</span> <span class="nf">Y_3_6</span><span class="p">(</span> <span class="k">const</span> <span class="n">V3_t</span><span class="o">&amp;</span> <span class="n">n</span> <span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="p">(</span> <span class="o">-</span><span class="n">m_k06</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span>     <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">x</span> <span class="p">)</span> <span class="o">-</span> <span class="p">(</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">n</span><span class="p">.</span><span class="n">y</span> <span class="p">)</span> <span class="p">)</span> <span class="p">);</span> <span class="p">}</span>
</code></pre></div></div>

<p>This spherical (<em>i.e.,</em> polar in 3D) plot represents the SH basis for the first four levels (L0-L1-L2-L3). The SH magnitude is used for the radius at each spherical coord and the sign is used for the color.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics.jpg" alt="L0-L1-L2-L3 Spherical Harmonics" /></p>

<p>This is the same type of plot, but fixating \(r=1\) and using the magnitude/sign to interpolate between both colors. Note that I have thresholded the magnitude a bit so sign-flip boundaries more sharply distinct.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-unit.jpg" alt="Trajectory constraint" /></p>

<p>This is again the same plot, but now each unit sphere is unfolded in a cubemap layout. The same color thresholding is used here.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-cubemap.jpg" alt="Trajectory constraint" /></p>

<h3 id="what-the-shs-will-do-for-us">What the SHs will do for us</h3>

<p>Recap: The radiance arriving at a point from all directions is a fundamental building block in many real-time GI approximation algorithms. Let’s use the term light probe from now on.</p>

<p>A light probe stores what a point in space <em>sees</em> if it <em>looks around</em>. In other words, a light probe is nothing but a <em>render of the scene</em> with a <em>spherical cam</em> centered at the point.</p>

<p>The radiance arriving at a point from all directions is precisely a (color) function defined on the surface of a sphere centered at the point. Thus, we can encode arriving directional radiance with SHs.</p>

<h2 id="encodingdecoding">Encoding/Decoding</h2>

<p>Let’s define some terminology here:</p>

<p>L: Number of SH levels we will use.</p>

<p>Each level contributes \(L+1+L\) SHs.</p>

<p>So, for example, if we choose \(L=4\), then the total number of SHs involved will be \(N_{SH}=1+3+5+7=16\). It can be easily proven that for any \(L\), the total number of SHs will be \(N_{SH}=L^2\).</p>

<p><strong>Encoding:</strong> The process of projecting the input panorama onto each of the \(N_{SH}\) harmonics. This process yields exactly \(N_{SH}\) real coefficients.</p>

<p><strong>Decoding:</strong> The process of restoring the input panorama with the linear combination of those \(N_{SH}\) real coefficients each multiplied by its corresponding harmonic.</p>

<p><strong>SH codec:</strong> This is the shortname with which we will refer to the process of encoding-and-decoding an input panorama.</p>

<p>Naturally, the higher the number of levels \(L\) the lower the loss of information. But also the number of coefficients (storage and mathematical operations required) will grow quadratically.</p>

<p>For light probes in real-time \(L=3\) or \(L=4\) are usual choices. Radiance arriving at a point is hoefully low-frequency, so few coefficients offer a good compromise between detail preservation and storage size.</p>

<p>It goes without saying, but the SH codec is called once per color component (<em>i.e.,</em> \(3\) times per component). This also means that the actual number of real coefficients for a color input panorama is \(3L^2\).</p>

<h3 id="ldr-panorama">LDR panorama</h3>

<p>Let’s start with this panorama taken from <a href="https://commons.wikimedia.org/wiki/File:Harderwijk_harbour_2018_-_360_panorama.jpg">commons.wikimedia.org</a>. The image is 8-bit, Low-Dynamic-Range, and not particularly high-frequency. <em>i.e.,</em> a 360-degree panorama of a pretty exterior.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr.png" alt="LDR panorama" /></p>

<p>Here’s what happens if we pass it through our SH codec with \(L=3\) (\(9 \times 3=27\) coefficients).</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr-9x3.png" alt="SH reconstruction LDR 27" /></p>

<p>Now with \(L=4\) (\(16 \times 3=48\) coefficients).</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr-16x3.png" alt="SH reconstruction LDR 48" /></p>

<p>And now with \(L=10\) (\(100 \times 3=300\) coefficients).</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr-100x3.png" alt="SH reconstruction LDR 300" /></p>

<blockquote>
  <p>You can right-click and download (or open in another tab) to view these images at 1:1 scale.</p>
</blockquote>

<p>The more the SHs (and coefficients) the more faithful and detailed the reconstruction becomes. While this is obvious, it is important to remark that the SH basis we’re using is “ordered by frequency”, meaning that the first ones encode lower-frequency bands, and adding more and more SHs adds more and more higher-frequency bands. In the continuous case, an infinite number of SHs and coefficients would be required to guarantee an ever lossless restoration. In the discrete case, as with the FT and other transforms, a finite amount of coefficients (proportional to the input data size) will suffice.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr-progression.gif" alt="SH reconstruction progression" /></p>

<p>Let’s try with a spherically-blurred version (not a regular image blur):</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-ldr-blurred-progression.gif" alt="SH reconstruction progression" /></p>

<p>The most important takeaway here is:</p>

<blockquote>
  <p>With just a handful of floating-point coefficients we are able to reconstruct a decent low-frequency approximation to an arbitrarily large-resolution panoramic view of a scene. The encoding/decoding process is defined by a very compact and fairly efficient formulation.</p>
</blockquote>

<p>Now, how well-behaved an SH codec is? Does it also work well with more complicated data?</p>

<h3 id="hdr-panorama">HDR panorama</h3>

<p>Since an SH codec transforms data from its natural space domain to the frequency domain, one can reasonably expect that the type of frequencies found in the original data will greatly affect the amount of SH levels (coefficients) necessary to reconstruct data more or less faithfully.</p>

<p>Meaning that data with higher frequencies will be encoded at a greater loss. This sounds intuitive. But there is other less obvious and more harmful implication. As soon as high frequencies appear in the data, artifacts (and not just a loss of detail) will start to appear. <em>Ringing</em> in particular.</p>

<p>This is nothing but a manifestation of the well-known <a href="https://en.wikipedia.org/wiki/Gibbs_phenomenon">Gibbs phenomenon</a>.</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-hdr-1.gif" alt="SH reconstruction progression" /></p>

<p>Higher frequencies in an image semantically mean more fine/sharp detail (<em>i.e.,</em> more average variation from one pixel to its neighbors). But another factor that amplifies the amount of variation is the dynamic range of the data. Having a high-dynamic range may or may not affect what frequencies are present, but will certainly affect the amplitudes (the magnitude) of the encoded coefficients. Since the harmonics are wave-like, ringing may occur.</p>

<p>The only true solution to this problem would be to keep adding more and more levels/coefficients. But that is obviously not an option. So we may be forced to cheat a little:</p>

<ul>
  <li>Reduce or clamp the dynamic range (HDR-&gt;LDR).</li>
  <li>Blur the input data to shave off the higher frequencies.</li>
</ul>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-hdr-clamped.gif" alt="SH reconstruction progression" /></p>

<p>Let’s try with another classic HDR panorama. High-frequencies and high-amplitudes together cause a wobbly disaster:</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-hdr-2.gif" alt="SH reconstruction progression" /></p>

<p>And now let’s try with a spherically-blurred version of the same map. The particular blur I am using here is a cosine-weighted hemispherical blur:</p>

<p><img src="/uploads/2025/spherical-harmonics/chemaguerra-spherical-harmonics-hdr-blurred.gif" alt="SH reconstruction progression" /></p>

<p>Because type of blur used is cosine-weighted over the hemisphere, this light probe exactly captures the irradiance that a <em>lambertian surface</em> would receive from the scene if illuminated with the unblurred HDR panorama.</p>

<p>And this is precisely what SH encoding/decoding is good for:</p>

<h2 id="valuable-resources">Valuable resources</h2>

<p>These are “mandatory” resources with great in-depth explanations:</p>

<ul>
  <li><a href="http://www.ppsloan.org/publications/StupidSH36.pdf">Stupid Spherical Harmonics Tricks</a> by Peter-Pike Sloan.</li>
  <li><a href="https://graphics.stanford.edu/papers/envmap/">An Efficient Representation for Irradiance Environment Maps</a> by Ramamoorthi and Hanrahan.</li>
  <li><a href="http://www.cse.chalmers.se/~uffe/xjobb/Readings/GlobalIllumination/Spherical%20Harmonic%20Lighting%20-%20the%20gritty%20details.pdf">Spherical Harmonic Lighting: The Gritty Details</a> by Robin Green.</li>
</ul>]]></content><author><name>Chema Guerra</name></author><summary type="html"><![CDATA[Note: This post was written nearly 2 years ago and never published. I’m posting it now as-is, with some sections incomplete but the core content intact. This post is a quick crash course on what Spherical Harmonics are and how they can be used to efficiently encode and decode incoming radiance (i.e., a spherical view of the scene) at a point. Motivation Radiance and irradiance In the context of rendering: Radiance (\(L\)) is the amount of light that flows through or is emitted from a point in space in a particular direction. Irradiance (\(E\)) is the total amount of light energy incident upon a surface at a specific point. \[E=\int_{\Omega}{L(\mathbf{\omega})cos(\theta)\mathrm{d}\mathbf{\omega}}\] Where: \(E\) is the irradiance at a point on the surface. \(L(\mathbf{\omega})\) is the radiance arriving from direction \(\mathbf{\omega}\). \(\theta\) is the angle between the surface normal and the direction \(\mathbf{\omega}\). \(d\mathbf{\omega}\) represents the differential solid angle in the hemisphere of incoming light directions. Light probes The definition above talks about surfaces (with a normal). That is, radiance arriving at the point from the positive side of the surface. However, in this post we are interested on the radiance arriving at a point from all directions: \[E=\int_{O}{L(\mathbf{\omega})\mathrm{d}\mathbf{\omega}}\] Under certain circumstances we may want to precompute-and-store the incoming radiance \(L\) at a point or a collection of points in the scene in order to approximate illumination dynamically in real-time (possibly making use of \(E\) somehow). \(L\) is a function defined on the sphere, and is called light probe… \(E\) shows up in the Rendering Equation but again, in this post we’re only interested in an efficient way to precompute-and-store \(L(\mathbf{\omega})\). In a real scenario, light probes can be placed either manually or automatically and they are meant to suffciently cover the scene. The goal of each individual light probe is to keep track of what illumination looks like in the volumetric neighborhood of the point they are centered at. Typically, nearby light probes are sampled and the samples obtained are interpolated to figure out the illumination arriving at any given 3D point. This can give a decent and fast-to-query approximation to Global Illumination and/or soft shadowing. The information stored by a light probe can be pictured of as a panoramic view of what the point sees around itself. So typical ways to initialize (or update) a light probe are to literally render/rasterize the scene around (maybe with 6 cubemap-arranged cameras) or ray-tracing the surroundings stochastically. Whichever the method, the resulting irradiance is a spherical collection of colors-and-their-magnitudes. Or, in better words, a function defined on the sphere which yields a color when evaluated at each unit direction. Which, in more humane terms, is a \(360\deg\) photograph of the scene from the point (a panorama or a cubemap if you prefer). Those are usually rather heavy, and since scenes typically require (many) thousands of light probes to sufficiently cover their volume, finding a highly efficient and performant (albeit lossless) way to encode such spherical color maps sounds ideal. It is important to note that such radiances-arriving-at-a-point maps can be very high-frequency as soon as we involve high-intensity point lights (such as the sun) and hard shadows. So in some engines all direct lighting coming from hard lights is calculated dynamically and light probes are used exclusively for indirect illumination and soft shadowing, which is much lower frequency. So let’s establish the goal of this post as: Find a highly efficient/performant way to encode (low-frequency) spherical color maps with as little loss as possible. Cubemaps Throughout this post I will display \(L(\mathbf{\omega})\) as cubemaps because cubemap projections exhibit little distortion compared to angular or latitude-longitude maps. It is important to note that in doing so, the integrals we will be calculating below need to account for distortion in their differential element. The rgb-coded cubemap represents all the unit directions in a sphere \(\mathbf{\omega}\). The grayscale cubemap represents the differential element conversion between the unit sphere and the unit cubemap \(\mathrm{d}\mathbf{\omega}\). The HDR map unfolded as a cubemap is an example of an incoming radiance map \(L(\mathbf{\omega})\). Sanity check: The sum of all the grayscale patches must be \(4\pi\) = the surface of the unit sphere. Spherical Harmonics SHs have no particular connection with rendering or radiance encoding. They are a generic mathematical contraption with which you can project some input data given in an input realm (usually space) onto another realm (usually frequency-related). This projection process is also reversible, which allows for encoding/decoding. This is all quite reminiscent of the Fourier Transform &amp; Co.. SH are particularly interesting in the context of Computer Graphics because their domain is the unit sphere, which makes them ideal to encode directional information, such as light probes as described above. Orthogonality and Key Properties Spherical Harmonics have several mathematical properties that make them ideal for encoding spherical functions: Orthogonality: The SH functions are orthogonal over the sphere. This property allows us to decompose any spherical function into independent components without interference between different harmonics. Completeness: Any square-integrable function on the sphere can be represented as a linear combination of spherical harmonics. For rendering, this means we can approximate any incoming radiance distribution to arbitrary precision by using enough SH coefficients. Rotation Invariance: The total power in each frequency band (each level \(l\)) is preserved under rotation. This makes SH coefficients stable when objects or light probes are rotated. Frequency Separation: Lower-order harmonics capture low-frequency (smooth) variations, while higher-order harmonics capture high-frequency (detailed) variations. This natural frequency ordering makes SH perfect for level-of-detail approximations. From Wikipedia: “Since the Spherical Harmonics form a complete set of orthogonal functions and thus an orthonormal basis, each function defined on the surface of a sphere can be written as a sum of these spherical harmonics.” What the SHs look like Laplace’s Spherical Harmonics are denoted \(Y_l^m(\omega)\) where: \(l\) is called the degree or order (ranging from 0 to infinity in theory, but we truncate for practical use). \(m\) is called the order within degree (ranging from \(-l\) to \(+l\)). For a given order \(l\), there are \(2l+1\) different harmonics, each identified by a different value of \(m\). This gives us: Level 0 (\(l=0\)): 1 harmonic Level 1 (\(l=1\)): 3 harmonics Level 2 (\(l=2\)): 5 harmonics Level 3 (\(l=3\)): 7 harmonics Total for \(L\) levels: \(L^2\) harmonics The harmonics are essentially different “shapes” or “patterns” on the sphere that form an orthogonal basis. Below is my implementation for the first 4 levels static constexpr T m_k01 = 0.2820947918; // sqrt( 1/pi)/2. static constexpr T m_k02 = 0.4886025119; // sqrt( 3/pi)/2. static constexpr T m_k03 = 1.0925484306; // sqrt( 15/pi)/2. static constexpr T m_k04 = 0.3153915652; // sqrt( 5/pi)/4. static constexpr T m_k05 = 0.5462742153; // sqrt( 15/pi)/4. static constexpr T m_k06 = 0.5900435860; // sqrt( 70/pi)/8. static constexpr T m_k07 = 2.8906114210; // sqrt(105/pi)/2. static constexpr T m_k08 = 0.4570214810; // sqrt( 42/pi)/8. static constexpr T m_k09 = 0.3731763300; // sqrt( 7/pi)/4. static constexpr T m_k10 = 1.4453057110; // sqrt(105/pi)/4. static T Y_0_0( const V3_t&amp; ) { return m_k01; } static T Y_1_0( const V3_t&amp; n ) { return ( -m_k02 * n.y ); } static T Y_1_1( const V3_t&amp; n ) { return ( m_k02 * n.z ); } static T Y_1_2( const V3_t&amp; n ) { return ( -m_k02 * n.x ); } static T Y_2_0( const V3_t&amp; n ) { return ( m_k03 * n.x * n.y ); } static T Y_2_1( const V3_t&amp; n ) { return ( -m_k03 * n.y * n.z ); } static T Y_2_2( const V3_t&amp; n ) { return ( m_k04 * ( ( 3 * n.z * n.z ) - 1 ) ); } static T Y_2_3( const V3_t&amp; n ) { return ( -m_k03 * n.x * n.z ); } static T Y_2_4( const V3_t&amp; n ) { return ( m_k05 * ( ( n.x * n.x ) - ( n.y * n.y ) ) ); } static T Y_3_0( const V3_t&amp; n ) { return ( -m_k06 * n.y * ( ( 3 * n.x * n.x ) - ( n.y * n.y ) ) ); } static T Y_3_1( const V3_t&amp; n ) { return ( m_k07 * n.z * ( ( n.y * n.x ) ) ); } static T Y_3_2( const V3_t&amp; n ) { return ( -m_k08 * n.y * ( ( 5 * n.z * n.z ) - ( 1 ) ) ); } static T Y_3_3( const V3_t&amp; n ) { return ( m_k09 * n.z * ( ( 5 * n.z * n.z ) - ( 3 ) ) ); } static T Y_3_4( const V3_t&amp; n ) { return ( -m_k08 * n.x * ( ( 5 * n.z * n.z ) - ( 1 ) ) ); } static T Y_3_5( const V3_t&amp; n ) { return ( m_k10 * n.z * ( ( n.x * n.x ) - ( n.y * n.y ) ) ); } static T Y_3_6( const V3_t&amp; n ) { return ( -m_k06 * n.x * ( ( n.x * n.x ) - ( 3 * n.y * n.y ) ) ); } This spherical (i.e., polar in 3D) plot represents the SH basis for the first four levels (L0-L1-L2-L3). The SH magnitude is used for the radius at each spherical coord and the sign is used for the color. This is the same type of plot, but fixating \(r=1\) and using the magnitude/sign to interpolate between both colors. Note that I have thresholded the magnitude a bit so sign-flip boundaries more sharply distinct. This is again the same plot, but now each unit sphere is unfolded in a cubemap layout. The same color thresholding is used here. What the SHs will do for us Recap: The radiance arriving at a point from all directions is a fundamental building block in many real-time GI approximation algorithms. Let’s use the term light probe from now on. A light probe stores what a point in space sees if it looks around. In other words, a light probe is nothing but a render of the scene with a spherical cam centered at the point. The radiance arriving at a point from all directions is precisely a (color) function defined on the surface of a sphere centered at the point. Thus, we can encode arriving directional radiance with SHs. Encoding/Decoding Let’s define some terminology here: L: Number of SH levels we will use. Each level contributes \(L+1+L\) SHs. So, for example, if we choose \(L=4\), then the total number of SHs involved will be \(N_{SH}=1+3+5+7=16\). It can be easily proven that for any \(L\), the total number of SHs will be \(N_{SH}=L^2\). Encoding: The process of projecting the input panorama onto each of the \(N_{SH}\) harmonics. This process yields exactly \(N_{SH}\) real coefficients. Decoding: The process of restoring the input panorama with the linear combination of those \(N_{SH}\) real coefficients each multiplied by its corresponding harmonic. SH codec: This is the shortname with which we will refer to the process of encoding-and-decoding an input panorama. Naturally, the higher the number of levels \(L\) the lower the loss of information. But also the number of coefficients (storage and mathematical operations required) will grow quadratically. For light probes in real-time \(L=3\) or \(L=4\) are usual choices. Radiance arriving at a point is hoefully low-frequency, so few coefficients offer a good compromise between detail preservation and storage size. It goes without saying, but the SH codec is called once per color component (i.e., \(3\) times per component). This also means that the actual number of real coefficients for a color input panorama is \(3L^2\). LDR panorama Let’s start with this panorama taken from commons.wikimedia.org. The image is 8-bit, Low-Dynamic-Range, and not particularly high-frequency. i.e., a 360-degree panorama of a pretty exterior. Here’s what happens if we pass it through our SH codec with \(L=3\) (\(9 \times 3=27\) coefficients). Now with \(L=4\) (\(16 \times 3=48\) coefficients). And now with \(L=10\) (\(100 \times 3=300\) coefficients). You can right-click and download (or open in another tab) to view these images at 1:1 scale. The more the SHs (and coefficients) the more faithful and detailed the reconstruction becomes. While this is obvious, it is important to remark that the SH basis we’re using is “ordered by frequency”, meaning that the first ones encode lower-frequency bands, and adding more and more SHs adds more and more higher-frequency bands. In the continuous case, an infinite number of SHs and coefficients would be required to guarantee an ever lossless restoration. In the discrete case, as with the FT and other transforms, a finite amount of coefficients (proportional to the input data size) will suffice. Let’s try with a spherically-blurred version (not a regular image blur): The most important takeaway here is: With just a handful of floating-point coefficients we are able to reconstruct a decent low-frequency approximation to an arbitrarily large-resolution panoramic view of a scene. The encoding/decoding process is defined by a very compact and fairly efficient formulation. Now, how well-behaved an SH codec is? Does it also work well with more complicated data? HDR panorama Since an SH codec transforms data from its natural space domain to the frequency domain, one can reasonably expect that the type of frequencies found in the original data will greatly affect the amount of SH levels (coefficients) necessary to reconstruct data more or less faithfully. Meaning that data with higher frequencies will be encoded at a greater loss. This sounds intuitive. But there is other less obvious and more harmful implication. As soon as high frequencies appear in the data, artifacts (and not just a loss of detail) will start to appear. Ringing in particular. This is nothing but a manifestation of the well-known Gibbs phenomenon. Higher frequencies in an image semantically mean more fine/sharp detail (i.e., more average variation from one pixel to its neighbors). But another factor that amplifies the amount of variation is the dynamic range of the data. Having a high-dynamic range may or may not affect what frequencies are present, but will certainly affect the amplitudes (the magnitude) of the encoded coefficients. Since the harmonics are wave-like, ringing may occur. The only true solution to this problem would be to keep adding more and more levels/coefficients. But that is obviously not an option. So we may be forced to cheat a little: Reduce or clamp the dynamic range (HDR-&gt;LDR). Blur the input data to shave off the higher frequencies. Let’s try with another classic HDR panorama. High-frequencies and high-amplitudes together cause a wobbly disaster: And now let’s try with a spherically-blurred version of the same map. The particular blur I am using here is a cosine-weighted hemispherical blur: Because type of blur used is cosine-weighted over the hemisphere, this light probe exactly captures the irradiance that a lambertian surface would receive from the scene if illuminated with the unblurred HDR panorama. And this is precisely what SH encoding/decoding is good for: Valuable resources These are “mandatory” resources with great in-depth explanations: Stupid Spherical Harmonics Tricks by Peter-Pike Sloan. An Efficient Representation for Irradiance Environment Maps by Ramamoorthi and Hanrahan. Spherical Harmonic Lighting: The Gritty Details by Robin Green.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-spherical-harmonics.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-spherical-harmonics.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Truncated SVD</title><link href="http://0.0.0.0:4000/2023/09/09/truncated-svd.html" rel="alternate" type="text/html" title="Truncated SVD" /><published>2023-09-09T10:58:40+02:00</published><updated>2023-09-09T10:58:40+02:00</updated><id>http://0.0.0.0:4000/2023/09/09/truncated-svd</id><content type="html" xml:base="http://0.0.0.0:4000/2023/09/09/truncated-svd.html"><![CDATA[<p>NOTE: This write-up is not finished yet…</p>

<h2 id="pca">PCA</h2>

<p>https://en.wikipedia.org/wiki/Principal_component_analysis</p>

<p>This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data.</p>

<p>Without getting into the details, this is done via an eigen-decomposition of the <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a>.</p>

<p><img src="/uploads/2023/chemaguerra-pca.gif" width="600" height="240" alt="Principal Component Analysis" /></p>

<h2 id="svd">SVD</h2>

<p><em>Singular Value Decomposition</em> (<a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a>) is a matrix factorization technique that factors a real matrix <code class="language-plaintext highlighter-rouge">M</code> into three matrices <code class="language-plaintext highlighter-rouge">U</code>, <code class="language-plaintext highlighter-rouge">Σ</code>, and <code class="language-plaintext highlighter-rouge">V</code> such that <code class="language-plaintext highlighter-rouge">M=U*Σ*V_T</code>.</p>

<p>If <code class="language-plaintext highlighter-rouge">M</code> is <code class="language-plaintext highlighter-rouge">mxn</code>, then <code class="language-plaintext highlighter-rouge">U</code> is <code class="language-plaintext highlighter-rouge">mxm</code>, <code class="language-plaintext highlighter-rouge">Σ</code> is <code class="language-plaintext highlighter-rouge">mxn</code> and <code class="language-plaintext highlighter-rouge">V</code> is <code class="language-plaintext highlighter-rouge">nxn</code>. Both <code class="language-plaintext highlighter-rouge">U</code> and <code class="language-plaintext highlighter-rouge">V</code> are orthonormal, and <code class="language-plaintext highlighter-rouge">Σ</code> is rectangular-diagonal with non-negative coefficients.</p>

<p>This is very similar to PCA, excepting that the factorization for SVD is done on the data matrix, whereas for PCA, the factorization is done on the covariance matrix.</p>

<p>The diagonal coefficients of S are known as the <em>singular values</em> of <code class="language-plaintext highlighter-rouge">M</code> and it is common practice to rearrange the SVD so the singular values are given in decreasing order. The number of non-zero singular values is equal to the rank of <code class="language-plaintext highlighter-rouge">M</code>.</p>

<p>The SVD is tightly <a href="https://intoli.com/blog/pca-and-svd/">related to PCA</a>:</p>

<ul>
  <li>The columns of <code class="language-plaintext highlighter-rouge">V</code> are principal directions/axes (eigenvectors).</li>
  <li>Columns of <code class="language-plaintext highlighter-rouge">U*Σ</code> are principal components (scores).</li>
  <li>Singular values are related to the eigenvalues of the covariance matrix.</li>
</ul>

<p>From Wikipedia:</p>

<p><img src="/uploads/2023/chemaguerra-svd-matrices.png" width="440" height="513" alt="SVD matrices" /></p>

<h2 id="truncated-svd">Truncated SVD</h2>

<p>Let’s try with a 1024x1024 grayscale image of the moon:</p>

<p><img src="/uploads/2023/chemaguerra-moon.png" width="600" height="600" alt="Truncated SVD (moon)" /></p>

<p>Such an image can be interpreted as 1024 vectors of 1024 components each. <em>i.e.,</em> a set of 1024 vectors in a 1024-dimension space.</p>

<p>If we run PCA/SVD on this set, the three matrices <code class="language-plaintext highlighter-rouge">U</code>, <code class="language-plaintext highlighter-rouge">Σ</code>, <code class="language-plaintext highlighter-rouge">V</code> will be 1024x1024. In particular, <code class="language-plaintext highlighter-rouge">Σ</code> will be a square-diagonal matrix. <em>i.e.,</em> only the coefficients in the diagonal are potentially non-zero. It is common practice to rearrange the three matrices so the diagonal indices in <code class="language-plaintext highlighter-rouge">Σ</code> are sorted from greater (top-left) to lower (bottom-right).</p>

<p><em>Truncated SVD</em> is simply the act of zeroing-out all the coefficients in <code class="language-plaintext highlighter-rouge">Σ</code> except for the top-left <code class="language-plaintext highlighter-rouge">n</code> ones.</p>

<p>Coefficient truncation implies that we’re also trashing <code class="language-plaintext highlighter-rouge">1024-n</code> columns in <code class="language-plaintext highlighter-rouge">U</code> and in <code class="language-plaintext highlighter-rouge">V</code> (as now those will be multiplied by 0 anyway).</p>

<p>If we now reconstruct the original matrix <code class="language-plaintext highlighter-rouge">M'=U'*Σ'*V_T'</code> using the truncated matrices, we will obtain <code class="language-plaintext highlighter-rouge">M'</code>, which will resemble <code class="language-plaintext highlighter-rouge">M</code>. The fewer the coefficients that we drop, the more closely that <code class="language-plaintext highlighter-rouge">M'</code> will approximate <code class="language-plaintext highlighter-rouge">M</code>. But because of the information-preserving properties of PCA/SVD, keeping just a bunch of the topmost coefficients in <code class="language-plaintext highlighter-rouge">Σ</code> may suffice to restore all (or near all) the original information.</p>

<h2 id="here-goes-a-demo">Here goes a demo</h2>

<ul>
  <li>The left half is the reconstructed matrix <code class="language-plaintext highlighter-rouge">M'</code>.</li>
  <li>The right half is the reconstruction error <code class="language-plaintext highlighter-rouge">abs(M-M')</code>.</li>
  <li>The decreasing yellow graph is the MSE as fewer and fewer singular values are zeroed-out.</li>
</ul>

<p><img src="/uploads/2023/chemaguerra-moon-svd.jpg" width="600" height="300" alt="Truncated SVD (moon)" /></p>

<p>This screenshot is <code class="language-plaintext highlighter-rouge">M'</code> reconstructed with only 32 (out of 1024) components.  <em>Right click + Open in new tab</em> for 1:1 quality.</p>

<p>Below is a video with the same image pair as more and more components are used for reconstruction. Most of the action happens in the first few frames.</p>

<p><img src="/uploads/2023/chemaguerra-moon-sequence.png" width="600" height="200" alt="Truncated SVD sequence (moon)" /></p>

<div class="embed-container">
  <iframe width="640" height="390" src="https://www.youtube.com/embed/6kzaotZlEWo" frameborder="0" allowfullscreen=""></iframe>
</div>
<style>
.embed-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
  max-width: 100%;
}
.embed-container iframe,
.embed-container object,
.embed-container embed {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>

<p><br /></p>

<p>What’s remarkable here (the magic of PCA/SVD) is how quickly the error graph decreases. This proves that the first components capture most of the information present in <code class="language-plaintext highlighter-rouge">M</code>, while the trailing components only carry high-frequency/low-amplitude fine details.</p>

<p>This is reminiscent of what happens with the Fourier Transform, the Cosine/Sine Transform and such. Those transforms deal with the space vs. frequency duality, whereas PCA/SVD is purely a variance-driven change-of-basis. But in a similar fashion, all these methods transform information to a dual form where the “amount of information” emerges in a structured, manageable way.</p>

<p>The FT/CT/etc… lie at the foundation of <em>.jpeg</em>, <em>.mp3</em> and other compression systems which exploit the fact that the Human Perception System is more sensitive to luminance (vs. chromaticity), and to lower (vs. higher) frequencies.</p>

<p>In the case of SVD/PCA, the upper coefficients in <code class="language-plaintext highlighter-rouge">Σ</code> capture more data variance than the lower ones.</p>

<p>These sequences are reconstructions with 2, 4, 8, 16, 32, 64, and 128 coefficients.</p>

<p><img src="/uploads/2023/chemaguerra-boi-sequence.png" width="600" height="200" alt="Truncated SVD sequence (boi)" /></p>

<div class="embed-container">
  <iframe width="640" height="390" src="https://www.youtube.com/embed/dOujW5C-A2A" frameborder="0" allowfullscreen=""></iframe>
</div>
<style>
.embed-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
  max-width: 100%;
}
.embed-container iframe,
.embed-container object,
.embed-container embed {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>

<p><br /></p>

<h2 id="easier-vs-harder-cases">Easier vs. harder cases</h2>

<p>As explained above, the matrix factorization can be interpreted as a change of basis to a special space where the data has rows which are linearly dependent with each other. In such case, <code class="language-plaintext highlighter-rouge">U/V</code> matrices of a lower rank will suffice to reconstruct the original matrix exactly. Actually, <code class="language-plaintext highlighter-rouge">Σ</code> will present itself with as many zero-valued coefficients in its diagonal as rows/columns can be trashed without causing any loss of data.</p>

<p>An extreme case is presented below (a centered square box shape), where 1 coefficient/row/col suffices. In this case both inside and outside the shape, all rows/cols are identical. A box is a separable convolution filter, BTW (future post on low-rank convolution incoming, I hope).</p>

<p><img src="/uploads/2023/chemaguerra-rect-sequence.png" width="600" height="200" alt="Truncated SVD (rect)" /></p>

<p>Rotating the shape brings disaster despite PCA/SVD are capable of “auto-detecting” such changes of basis. But here we’re dealing with discrete math, so the rotated shape gets “pixelated” and this makes the decomposition become numerically impure.</p>

<p>It’s funny to see how in the first frames of the video the reconstruction “insists” on being an unrotated square, somehow.</p>

<p><img src="/uploads/2023/chemaguerra-tilt-sequence.png" width="600" height="200" alt="Truncated SVD (tilted rect)" /></p>

<video controls="controls" playsinline="playsinline" loop="" src="/uploads/2023/chemaguerra-tilt-256.mp4" width="600" height="300" poster="/uploads/2023/chemaguerra-tilt-poster.png" alt="Truncated SVD (tilt rect)" preload="none"></video>

<p>Below, a pentagonal shape.</p>

<p><img src="/uploads/2023/chemaguerra-penta-sequence.png" width="600" height="200" alt="Truncated SVD (pentagon)" /></p>

<video controls="controls" playsinline="playsinline" loop="" src="/uploads/2023/chemaguerra-penta-256.mp4" width="600" height="300" poster="/uploads/2023/chemaguerra-penta-poster.png" alt="Truncated SVD (pentagon)" preload="none"></video>

<h2 id="practical-uses">Practical uses</h2>

<p>There are interesting practical uses for SVD truncation other than dimensionality reduction in data analysis.</p>

<h2 id="data-compression">Data compression</h2>

<p><a href="https://bartwronski.com/2020/05/21/dimensionality-reduction-for-image-and-texture-set-compression/">Bart Wronski</a> has a very interesting write up on <em>compression of PBR texture sets</em> using this technique.</p>

<p><em>BCn Texture Compression</em> is based on dimensionality reduction as well. Nice write up on the subject by <a href="https://www.reedbeta.com/blog/understanding-bcn-texture-compression-formats/">Nathan Reed</a>.</p>

<h2 id="low-rank-approximation">Low-rank approximation</h2>

<p>(Low-rank approximation)[https://en.wikipedia.org/wiki/Low-rank_approximation].</p>

<p>Another interesting read by <a href="https://bartwronski.com/2020/02/03/separate-your-filters-svd-and-low-rank-approximation-of-image-filters/">Bart Wronski</a>. I wish to do my own write up on <em>low-rank convolution</em> soon.</p>

<h2 id="implementation-details">Implementation details</h2>

<p>I had some old PCA/SVD C++ code in <a href="https://maverickrender.com/">Maverick</a>’s API, which I used for the images/videos in this post. But after reading this post by <a href="https://blog.demofox.org/2022/07/12/calculating-svd-and-pca-in-c/">Atrix256</a> I may bite the bullet and replace the implementation part of my old <code class="language-plaintext highlighter-rouge">xsvd_c</code> class with <a href="https://eigen.tuxfamily.org/">Eigen</a>.</p>]]></content><author><name>Chema Guerra</name></author><summary type="html"><![CDATA[NOTE: This write-up is not finished yet… PCA https://en.wikipedia.org/wiki/Principal_component_analysis This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Without getting into the details, this is done via an eigen-decomposition of the covariance matrix. SVD Singular Value Decomposition (SVD) is a matrix factorization technique that factors a real matrix M into three matrices U, Σ, and V such that M=U*Σ*V_T. If M is mxn, then U is mxm, Σ is mxn and V is nxn. Both U and V are orthonormal, and Σ is rectangular-diagonal with non-negative coefficients. This is very similar to PCA, excepting that the factorization for SVD is done on the data matrix, whereas for PCA, the factorization is done on the covariance matrix. The diagonal coefficients of S are known as the singular values of M and it is common practice to rearrange the SVD so the singular values are given in decreasing order. The number of non-zero singular values is equal to the rank of M. The SVD is tightly related to PCA: The columns of V are principal directions/axes (eigenvectors). Columns of U*Σ are principal components (scores). Singular values are related to the eigenvalues of the covariance matrix. From Wikipedia: Truncated SVD Let’s try with a 1024x1024 grayscale image of the moon: Such an image can be interpreted as 1024 vectors of 1024 components each. i.e., a set of 1024 vectors in a 1024-dimension space. If we run PCA/SVD on this set, the three matrices U, Σ, V will be 1024x1024. In particular, Σ will be a square-diagonal matrix. i.e., only the coefficients in the diagonal are potentially non-zero. It is common practice to rearrange the three matrices so the diagonal indices in Σ are sorted from greater (top-left) to lower (bottom-right). Truncated SVD is simply the act of zeroing-out all the coefficients in Σ except for the top-left n ones. Coefficient truncation implies that we’re also trashing 1024-n columns in U and in V (as now those will be multiplied by 0 anyway). If we now reconstruct the original matrix M'=U'*Σ'*V_T' using the truncated matrices, we will obtain M', which will resemble M. The fewer the coefficients that we drop, the more closely that M' will approximate M. But because of the information-preserving properties of PCA/SVD, keeping just a bunch of the topmost coefficients in Σ may suffice to restore all (or near all) the original information. Here goes a demo The left half is the reconstructed matrix M'. The right half is the reconstruction error abs(M-M'). The decreasing yellow graph is the MSE as fewer and fewer singular values are zeroed-out. This screenshot is M' reconstructed with only 32 (out of 1024) components. Right click + Open in new tab for 1:1 quality. Below is a video with the same image pair as more and more components are used for reconstruction. Most of the action happens in the first few frames. What’s remarkable here (the magic of PCA/SVD) is how quickly the error graph decreases. This proves that the first components capture most of the information present in M, while the trailing components only carry high-frequency/low-amplitude fine details. This is reminiscent of what happens with the Fourier Transform, the Cosine/Sine Transform and such. Those transforms deal with the space vs. frequency duality, whereas PCA/SVD is purely a variance-driven change-of-basis. But in a similar fashion, all these methods transform information to a dual form where the “amount of information” emerges in a structured, manageable way. The FT/CT/etc… lie at the foundation of .jpeg, .mp3 and other compression systems which exploit the fact that the Human Perception System is more sensitive to luminance (vs. chromaticity), and to lower (vs. higher) frequencies. In the case of SVD/PCA, the upper coefficients in Σ capture more data variance than the lower ones. These sequences are reconstructions with 2, 4, 8, 16, 32, 64, and 128 coefficients. Easier vs. harder cases As explained above, the matrix factorization can be interpreted as a change of basis to a special space where the data has rows which are linearly dependent with each other. In such case, U/V matrices of a lower rank will suffice to reconstruct the original matrix exactly. Actually, Σ will present itself with as many zero-valued coefficients in its diagonal as rows/columns can be trashed without causing any loss of data. An extreme case is presented below (a centered square box shape), where 1 coefficient/row/col suffices. In this case both inside and outside the shape, all rows/cols are identical. A box is a separable convolution filter, BTW (future post on low-rank convolution incoming, I hope). Rotating the shape brings disaster despite PCA/SVD are capable of “auto-detecting” such changes of basis. But here we’re dealing with discrete math, so the rotated shape gets “pixelated” and this makes the decomposition become numerically impure. It’s funny to see how in the first frames of the video the reconstruction “insists” on being an unrotated square, somehow. Below, a pentagonal shape. Practical uses There are interesting practical uses for SVD truncation other than dimensionality reduction in data analysis. Data compression Bart Wronski has a very interesting write up on compression of PBR texture sets using this technique. BCn Texture Compression is based on dimensionality reduction as well. Nice write up on the subject by Nathan Reed. Low-rank approximation (Low-rank approximation)[https://en.wikipedia.org/wiki/Low-rank_approximation]. Another interesting read by Bart Wronski. I wish to do my own write up on low-rank convolution soon. Implementation details I had some old PCA/SVD C++ code in Maverick’s API, which I used for the images/videos in this post. But after reading this post by Atrix256 I may bite the bullet and replace the implementation part of my old xsvd_c class with Eigen.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-truncated-svd.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-truncated-svd.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Principal Component Analysis</title><link href="http://0.0.0.0:4000/2023/09/07/principal-component-analysis.html" rel="alternate" type="text/html" title="Principal Component Analysis" /><published>2023-09-07T10:58:40+02:00</published><updated>2023-09-07T10:58:40+02:00</updated><id>http://0.0.0.0:4000/2023/09/07/principal-component-analysis</id><content type="html" xml:base="http://0.0.0.0:4000/2023/09/07/principal-component-analysis.html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> is a <em>data transformation</em> technique.</p>

<p>Given an n-dimensional set of data points, it finds an n-dimensional orthonormal basis where our data points can hopefully be seen……..</p>

<p>Besides computing the n basis axes, PCA also computes n values which tell how much of the total variance in the dataset is captured by each axis.</p>

<p>EXTEND</p>

<h2 id="example-2d-dataset">Example 2D dataset</h2>

<p>To illustrate this post we will be using a 2D dataset made of 1024 points uniformly distributed in the shape of an ellipse of radii \(a=1\) and \(b=.25\). The ellipse is centered at \((.5,.5)\) and tilted by \(\frac{\pi}{6}\) radians.</p>

<p><img src="/uploads/2023/chemaguerra-pca-dataset.png" alt="PCA example dataset" /></p>

<h2 id="simple-linear-regression">Simple Linear Regression</h2>

<p>First, let’s detour a little to talk about <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">Simple Linear Regression</a>, which is a basic method one can use to approximate a set of 2D data points with a <em>best fitting line</em>.</p>

<p>It is common practice to define the best fitting line using <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a>. <em>i.e.,</em> the line that minimizes the sum of vertical distances between itself and the points in the dataset.</p>

<p>I hope that the below video depicts this idea clearly:</p>

<video controls="controls" playsinline="playsinline" loop="" src="/uploads/2023/chemaguerra-slr-ellipse-512.mp4" width="512" height="512" poster="/uploads/2023/chemaguerra-slr-ellipse-512-poster.png" alt="Linear Regression" preload="none"></video>

<p>The vertical distances (also called <em>residuals</em>) are in yellow. As the best fitting line candidate (in white) rotates the plot at the bottom represents the OLS sum. It can be seen that the sum goes to infinity when the line is vertical, but reaches a minimum when the line is <em>as tilted as</em> the dataset.</p>

\[\begin{align}
\widehat\beta&amp;=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})(x_i-\bar{x})} \\
\widehat\alpha&amp;=\bar{y}-\widehat\beta{\bar{x}}
\end{align}\]

<p>In our dataset these calculate to \(\widehat\alpha=.238\ldots\) and \(\widehat\beta=.523\ldots\) which we can plug in the line equation:</p>

\[y=\widehat\alpha+\widehat\beta{x}\]

<p>\(.523\ldots=\widehat\beta\simeq tan(\frac{\pi}{6})=.577\ldots\) which means that the slope of the best fitting line is about \(\frac{\pi}{6}\) radians, like our dataset. All good.</p>

<p><img src="/uploads/2023/chemaguerra-slr-ellipse-result.png" alt="SLR result" /></p>

<p>The point of this detour is to introduce SLR as a basic intuitive method to capture some meaningful feature of an arbitrary dataset using a much simpler instrument such as a line.</p>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>

<video controls="controls" playsinline="playsinline" loop="" src="/uploads/2023/chemaguerra-pca-ellipse-512.mp4" width="512" height="512" poster="/uploads/2023/chemaguerra-pca-ellipse-512-poster.png" alt="Linear Regression" preload="none"></video>

<p><img src="/uploads/2023/chemaguerra-pca-ellipse-result.png" alt="PCA result" /></p>

<p>EXTEND ALGORITHM</p>

<h2 id="pca-in-a-3d-point-cloud">PCA in a 3D point cloud</h2>

<p>EXTEND</p>

<h2 id="practical-uses-of-pca">Practical uses of PCA</h2>

<p>Most of the time, PCA is used as a preceding step to <em>data reduction</em>. Since PCA provides a set of axes where the dataset projections are sorted in decreasing order of variance, you can say that the amount of information provided by each new axes is less and less.</p>

<p>Actually, you may drop some of the trailing axes and obtain a <em>dimensionally-reduced</em> (and hence more tractable) version of your dataset that roughly captures the same amount of information as the original dataset.</p>

<h2 id="data-visualization">Data visualization</h2>

<p>A classic use is to drop all dimensions but the first two or three in order to plot the dataset in 2D or 3D. With a little luck, a heavy multi-dimensional dataset will exhibit some sort of obvious clustering you can make sense of when drawn on screen.</p>

<p>EXTEND WITH THE NUMBERS</p>

<h2 id="machine-learning">Machine Learning</h2>

<p>For ML, clustering, … it makes sense to run PCA on your data first. Then drop as many trailing principal components while still preserving the maximum amount of information (<em>e.g.</em> setting a cutoff at 99%). And from that moment on, work with the dimensionally-reduced dataset.</p>

<p>EXTEND WITH THE NUMBERS</p>

<h2 id="physics">Physics</h2>

<p>There is a beautiful relationship between the PCA of a set of points (interpreted as masses) and the tensor of inertia of said masses.</p>

<p>EXTEND</p>

<p>an axis of rotation is a principal axis, angular momentum and angular velocity are parallel, complex conjugate, importance analysis</p>

<p>EXTEND</p>

<h2 id="pca-implemented-in-eigen">PCA implemented in Eigen</h2>

<p>This is the implementation of PCA I am using in my code since I decided to replace some legacy implementations of numerical algorithms by Eigen.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Eigen::MatrixXd M;

M_to_E( m_M, __out__ M );  // Initialize M with the dataset.

const Eigen::VectorXd mean = M.colwise().mean();

M.rowwise() -= mean.transpose();

const Eigen::MatrixXd cov  = ( ( M.transpose() * M ) / ( M.rows() - 1 ) );

const Eigen::SelfAdjointEigenSolver&lt; Eigen::MatrixXd &gt; solver( cov );

if ( solver.info() == Eigen::Success )
{
  E_to_M( solver.eigenvalues (), __out__ m_E );  // Collect the solver's result.
  E_to_M( solver.eigenvectors(), __out__ m_V );	 //
}
</code></pre></div></div>

<h2 id="youtube-videos">YouTube videos</h2>

<div class="embed-container">
  <iframe width="640" height="390" src="https://www.youtube.com/embed/-xUIXzI2GY0" frameborder="0" allowfullscreen=""></iframe>
</div>
<style>
.embed-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
  max-width: 100%;
}
.embed-container iframe,
.embed-container object,
.embed-container embed {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>

<div class="embed-container">
  <iframe width="640" height="390" src="https://www.youtube.com/embed/xwusOZXPAWk" frameborder="0" allowfullscreen=""></iframe>
</div>
<style>
.embed-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
  max-width: 100%;
}
.embed-container iframe,
.embed-container object,
.embed-container embed {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>

<p><br />
PCA is very well documented on the Internet:</p>

<ul>
  <li>A great video by <a href="https://www.youtube.com/watch?v=TJdH6rPA-TI">Computerphile on PCA</a>.</li>
</ul>]]></content><author><name>Chema Guerra</name></author><summary type="html"><![CDATA[PCA is a data transformation technique. Given an n-dimensional set of data points, it finds an n-dimensional orthonormal basis where our data points can hopefully be seen…….. Besides computing the n basis axes, PCA also computes n values which tell how much of the total variance in the dataset is captured by each axis. EXTEND Example 2D dataset To illustrate this post we will be using a 2D dataset made of 1024 points uniformly distributed in the shape of an ellipse of radii \(a=1\) and \(b=.25\). The ellipse is centered at \((.5,.5)\) and tilted by \(\frac{\pi}{6}\) radians. Simple Linear Regression First, let’s detour a little to talk about Simple Linear Regression, which is a basic method one can use to approximate a set of 2D data points with a best fitting line. It is common practice to define the best fitting line using ordinary least squares. i.e., the line that minimizes the sum of vertical distances between itself and the points in the dataset. I hope that the below video depicts this idea clearly: The vertical distances (also called residuals) are in yellow. As the best fitting line candidate (in white) rotates the plot at the bottom represents the OLS sum. It can be seen that the sum goes to infinity when the line is vertical, but reaches a minimum when the line is as tilted as the dataset. \[\begin{align} \widehat\beta&amp;=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})(x_i-\bar{x})} \\ \widehat\alpha&amp;=\bar{y}-\widehat\beta{\bar{x}} \end{align}\] In our dataset these calculate to \(\widehat\alpha=.238\ldots\) and \(\widehat\beta=.523\ldots\) which we can plug in the line equation: \[y=\widehat\alpha+\widehat\beta{x}\] \(.523\ldots=\widehat\beta\simeq tan(\frac{\pi}{6})=.577\ldots\) which means that the slope of the best fitting line is about \(\frac{\pi}{6}\) radians, like our dataset. All good. The point of this detour is to introduce SLR as a basic intuitive method to capture some meaningful feature of an arbitrary dataset using a much simpler instrument such as a line. Principal Component Analysis EXTEND ALGORITHM PCA in a 3D point cloud EXTEND Practical uses of PCA Most of the time, PCA is used as a preceding step to data reduction. Since PCA provides a set of axes where the dataset projections are sorted in decreasing order of variance, you can say that the amount of information provided by each new axes is less and less. Actually, you may drop some of the trailing axes and obtain a dimensionally-reduced (and hence more tractable) version of your dataset that roughly captures the same amount of information as the original dataset. Data visualization A classic use is to drop all dimensions but the first two or three in order to plot the dataset in 2D or 3D. With a little luck, a heavy multi-dimensional dataset will exhibit some sort of obvious clustering you can make sense of when drawn on screen. EXTEND WITH THE NUMBERS Machine Learning For ML, clustering, … it makes sense to run PCA on your data first. Then drop as many trailing principal components while still preserving the maximum amount of information (e.g. setting a cutoff at 99%). And from that moment on, work with the dimensionally-reduced dataset. EXTEND WITH THE NUMBERS Physics There is a beautiful relationship between the PCA of a set of points (interpreted as masses) and the tensor of inertia of said masses. EXTEND an axis of rotation is a principal axis, angular momentum and angular velocity are parallel, complex conjugate, importance analysis EXTEND PCA implemented in Eigen This is the implementation of PCA I am using in my code since I decided to replace some legacy implementations of numerical algorithms by Eigen. Eigen::MatrixXd M; M_to_E( m_M, __out__ M ); // Initialize M with the dataset. const Eigen::VectorXd mean = M.colwise().mean(); M.rowwise() -= mean.transpose(); const Eigen::MatrixXd cov = ( ( M.transpose() * M ) / ( M.rows() - 1 ) ); const Eigen::SelfAdjointEigenSolver&lt; Eigen::MatrixXd &gt; solver( cov ); if ( solver.info() == Eigen::Success ) { E_to_M( solver.eigenvalues (), __out__ m_E ); // Collect the solver's result. E_to_M( solver.eigenvectors(), __out__ m_V ); // } YouTube videos PCA is very well documented on the Internet: A great video by Computerphile on PCA.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-principal-component-analysis.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-principal-component-analysis.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Constrained Dynamics (V)</title><link href="http://0.0.0.0:4000/2023/09/06/constrained-dynamics-v.html" rel="alternate" type="text/html" title="Constrained Dynamics (V)" /><published>2023-09-06T10:00:00+02:00</published><updated>2023-09-06T10:00:00+02:00</updated><id>http://0.0.0.0:4000/2023/09/06/constrained-dynamics-v</id><content type="html" xml:base="http://0.0.0.0:4000/2023/09/06/constrained-dynamics-v.html"><![CDATA[<p>This post is a continuation to the first entry in this series:</p>]]></content><author><name>Chema Guerra</name></author><summary type="html"><![CDATA[This post is a continuation to the first entry in this series:]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-constrained-dynamics-ii.png" /><media:content medium="image" url="http://0.0.0.0:4000/thumbnails/2023/chemaguerra-constrained-dynamics-ii.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>